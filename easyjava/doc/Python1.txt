1장.

자연 언어 처리(NLP)는 자연 언어와 컴퓨터 간의 상호작용에 관한 것이다. 자연 언어 처리는 인공 지능(AI)과 전산 언어학의 주요 구성 요소 중 하나이다. 그것은 컴퓨터와 인간 사이의 원활한 상호작용을 제공하고 컴퓨터가 머신 러닝의 도움으로 인간의 언어를 이해할 수 있는 기능을 제공한다. 프로그래밍 언어 파일의 내용 혹은 문서를 나타내는 데 사용되는 기본 데이터 타입(예를 들면, C, C ++, JAVA, Python 등)을 문자열로서 알려졌다. 이 장에서는 다양한 NLP 작업을 수행하는 것이 유용한 문자열에서 수행할 수 있는 다양한 작업을 살펴볼 것이다.

이 장에서는 다음 내용을 포함할 것이다.

-텍스트의 토큰
-텍스트의 정규화
-토큰의 교체 및 수정
-지프(Zipf)의 법칙을 텍스트에 적용
-Edit Distance 알고리즘을 사용한 유사도 적용(?)
-Jaccard의 Coefficient를 이용한 유사도 적용(?)
-스미스 워터맨(Smith Waterman)를 이용한 유사도 적용(?)

토큰화

토큰화는 토큰이라는 작은 부분으로 텍스트를 분할하는 과정으로 정의될 수 있으며, NLP에서는 중요한 단계로 간주된다.

2016.08.08

Page 2.

NLTK가 설치되고 파이썬 IDLE가 실행되면, 각각의 문장에 텍스트 혹은 단락의 토큰을 수행할 수 있다. 토큰화를 수행하기 위해, 문장 토큰 함수를 임포트할 수 있다. 이 함수의 인자는 토큰화되야하는 텍스트일 것이다. sent_tokenize 함수는 PunktSentenceTokenizer로 알려진 NLTK의 인스턴스를 사용한다. NLTK 인스턴스는 이미 문장의 시작과 끝을 표시하는 문자, 문장 부호에 기초하여 다른 유럽 언어로 토큰화를 수행하도록 훈련되었다.

문장을 텍스트로 토큰화(확인)

이제, 주어진 텍스트를 개별 문장으로 토큰화하는 방법을 살펴보자.

<소스>

따라서, 주어진 텍스트는 각각의 문장으로 분할된다. 또한, 각각의 문장에 대한 처리를 수행할 수 있다.

여러 문장을 토큰화하기 위해, PunktSentenceTokenizer를 로드하고 토큰화를 수행하기 위해 tokenize() 함수를 사용할 수 있다.

2016.06.09

이제, 각각의 문장으로 처리를 수행할 수 있다. 개별 문장은 단어로 토큰화된다. 단어 토큰화는 word_tokenize() 함수를 사용하여 수행된다. word_tokenize 함수는 단어 토큰화를 수행하는 TreebankWordTokenizer으로 알려진 NLTK 인스턴스를 사용한다.

여기에서는 word_tokenize를 사용한 영어 텍스트의 토큰화를 보여준다.

<소스>

단어의 토큰화는 TreebankWordTokenizer 로드와 단어로 토큰화될 필요가 있는 문장인 그 인자에서 tokenize() 함수의 호출에 의해서 수행될 수 있다. 이 NLTK의 인스턴스는 이미 공간과 문장 부호에 기초를 위한 단어로 문장의 토큰을 수행하기 위해 훈련을 하고있다.

다음 코드는 사용자의 입력을 얻고 토큰화하고 그 길이를 평가하도록 도와준다.

<소스>

Page 4.

TreebankWordTokenizer를 사용한 토큰화

이제 TreebankWordTokenizer를 사용하여 토큰화를 수행하는 코드를 살펴보자.

<소스>

TreebankWordTokenizer는 Penn Treebank Corpus에 따라 규칙을 사용한다. 축소로 분리하여 작동한다.(확인) 다음과 같다.

<소스>

또 다른 tokenizer는 PunktWordTokenizer 이다. 그것은 분할한 구두점으로 작동한다. 각 단어는 완전히 새로운 토큰을 생성하는 대신 유지된다. 또 다른 단어 tokenizer는 WordPunctTokenizer 이다. 완전히 새로운 토큰을 구둣점으로 분할하여 제공한다. 이 분할 타입은 보통 바람직하다.

<소스>

tokenizers의 상속 트리는 여기에 주어진다.

<그림>

2016.08.10

Page 5.

정규식을 사용한 토큰화

단어의 토큰은 두 가지 방법의 정규식을 구성하여 수행될 수 있다. 

-단어로 매칭
-공백 혹은 간격으로 매칭

NLTK에서 RegexpTokenizer를 임포트할 수 있다. 텍스트에 존재하는 토큰을 일치할 수 있는 정규식을 생성할 수 있다.

<소스>

인스턴스하는 클래스 대신에, 토큰화의 다른 방법은 이 함수를 사용하는 것이다.

<소스>

RegularexpTokenizer는 토큰을 매칭시켜 토큰화를 수행하는 re.findall() 함수를 사용한다. 틈새(gaps) 혹은 공간(space)를 매칭시켜 토큰화를 수행하는 re.split() 함수를 사용한다.

여백(whitespace)을 사용하여 토큰화하는 방법의 예제를 살펴 보자.

<소스>

대문자로 시작하는 단어를 선택하기 위해, 다음과 같은 코드가 사용된다.

Page 6.

다음 코드는 미리 정의된 정규식이 RegexpTokenizer의 서브클래스에서 사용하는 방법을 보여준다.

<소스>

스트링의 토큰화는 공백-탭, 스페이스, 또는 줄 바꿈(newline)을 사용하여 수행할 수 있다.

<소스>

WordPunctTokenizersms는 알파벳과 알파벳이 아닌 문자로 텍스트의 토큰을 수행하기 위한 정규 표현식  \w+|[^\w\s]+ 을 사용한다.

split() 메소드를 사용한 토큰화는 다음 코드에서 확인된다.

<소스>

sent.split('\n'), LineTokenizer와 유사하게, 라인에 텍스트를 토큰화하여 작동한다.

<소스>

Page 7.

2016.08.11

nltk.tokenize.util.string_span_tokenize(sent,separator)는 분리 기호의 각 범위에 분할하여 전송되는 토큰의 오프셋을 반환할 것이다.

<소스>

Page 8.

정규화

자연어 텍스트 프로세싱을 수행하기 위하여, 주로 구두점 제거, 소문자 또는 대문자로 전체 텍스트를 변환, 단어로 번호 변환, 약어 텍스트 정규화 확대 등 정규화를 수행할 필요가 있다.

구두점 제거

때때로, 토큰화 동안, 구두점 제거하는 것이 바람직하다. 구두점의 제거는 NLTK에서 정규화하는 동안 주요 작업 중 하나로 간주된다.

다음 예제를 살펴본다.

<소스>

앞의 코드는 토큰화된 텍스트를 가져온다. 다음 코드는 토큰화된 텍스트에서 문장 부호(구두점?)를 제거할 것이다.

<소스>

Page 9.

소문자와 대문자로 변환

주어진 텍스트는 lower() 및 upper() 함수 사용으로 완전하게 소문자 혹은 대문자 텍스트로 변환될 수 있다. 대문자 혹은 소문자로 텍스트를 변환하는 작업은 정규화의 카테고리에 해당된다.

변환의 다음 예제를 살펴본다.

<소스>

정지 단어 처리

Page 13.

RepeatReplacer 클래스는 컴파일하는 정규 표현식 및 대체 문자열에 의해 작동하고 replacers.py에 존재하는 backreference.Repeat_regexp를 사용하여 정의된다. 그것은 0(zero) 혹은 많은(\w*)가 되는 시작하는 문자, 0(zero) 혹은 많은(\w*)가 되는 종료 문자 혹은 같은 문자를 따르는 문자(\w)로 매치된다.

예를 들어, lotttt는 (lo)(t)t(tt)로 분할된다. 여기에서, 하나의 t를 줄이면, 스트링은 lottt가 된다. 연속적인 분리 과정, 마지막으로 얻어진 결과 스트링은 lot이다.

happy를 hapy로 변환하는 것에 대한 RepeatReplacer의 문제는 부적절하다. 이 문제를 방지하기 위해, 그것과 함께 wordnet을 포함할 수 있다.

이전에 생성한 replacers.py 프로그램에서, wordnet을 포함하는 다음 라인을 추가한다.

<소스>

Page 14.

이제, 이전에 언급한 문제를 극복할 수 있는 방법을 살펴 보자.

<소스>

동의어로 단어 교체

이제 우리는 동의어로 주어진 단어를 대체할 수 있는 방법을 살펴볼 것이다. 이미 존재하는 replacers.py를 위해, 단어와 동의어 간의 매핑을 제공하는 WordReplacer라는 클래스를 추가할 수 있다.

<소스>

동의어로 단어를 대체 예제

동의어로 단어를 대체하는 예제를 살펴 보자.

<소스>

Page 15.

2016.08.17

이 코드에서, replace() 함수는 word_map에서 단어에 해당하는 동의어를 찾는다. 동의어가 주어진 단어에 대하여 존재한다면, 그 단어는 동의어로 대체될 것이다. 주어진 단어에 대한 동의어가 존재하지 않으면, 대체(교체?)는 수행되지 않는다. 동일어는 반환될 것이다.

텍스트에 적용하는 지프의 법칙(Zipf's law)

텍스트에서 토큰의 빈번도를 말하는 지프의 법칙은 정렬된 목록에서의 순위 혹은 위치에 정비례한다. 이 법칙은 토큰이 언어로 배포하는 방법에 대해 설명한다. 일부 토큰은 매우 자주 발생하고, 일부는 중간 빈도로 발생, 일부 토큰은 거의 발생하지 않는다.

지프의 법칙을 기반으로 NLTK에서 log-log plot을 얻기 위한 코드를 살펴보자.

<소스>

앞의 코드는 문서에서 계급의 플롯(plot of rank) 대 단어의 빈도를 얻을 것이다.(확인) 그래서 지프의 법칙이 순위와 단어의 빈도 간의 비례 관계를 확인하여 모든 문서를 보유 여부를 확인할 수 있다.

Page 16.

유사도 측정

다수의 유사도는 NLP 작업을 수행하기 위해 사용할 수 있다. NLTK의 nltk.metrics 패키지는 각종 평가 혹은 다양한 NLP 작업을 수행하는 데 도움이 되는 유사도 측정을 제공하기 위해 사용된다.

NLP에서 taggers, chunkers 등의 성능을 테스트하기 위해, 정보 검색에서 검색 표준 점수는 사용될 수 있다.

명명된 개체 인식기의 출력이 교육 파일에서 얻은 표준 점수를 사용하여 분석할 수 있는 방법을 살펴 보자.

<소스>

편집 거리(Ethe edit distance) 알고리즘을 사용한 유사도 측정

두 스트링 사이에서 편집 거리(Edit distance) 또는 리벤슈타인 편집 거리(Levenshtein edit distance)는 두 스트링을 동일하게하기 위해 삽입, 교체, 혹은 삭제될 수 있는 문자의 수를 계산하기 위해 사용된다.

Edit Distance에서 수행되는 동작은 다음과 같다.

-Copying letters from the first string to the second string (cost 0) and yg g 
substituting a letter  with another (cost 1):
D(i-1,j-1) + d(si,tj)(Substitution / copy)

-Deleting a letter in the first string (cost 1)
D(i,j-1)+1  (삭제)

Page 17.

-두 번째 스트링에 문자를 삽입(cost 1)
D(i,j) = min D(i-1,j)+1  (삽입)

nltk.metrics 패키지에 포함되어 있는 Edit Distance의 파이썬 코드는 다음과 같다.

<소스>

Page 26.

<소스>

fourgrams을 생성하고 fourgrams의 빈도수를 생성하기 위해 다음과 같은 코드가 사용된다.

<소스>

이제 주어진 문장에 대한 ngrams을 생성하는 코드를 볼 수 있다.

<소스>

Page 27.

<소스>

주어진 텍스트를 위한 MLE 개발

MLE은 또한 다항식 논리 회귀 혹은 조건부 지수 분류기로서 언급되는 NLP 분야에서 필수적인 작업이다. 그것은 버거(Berger)와 델라 트에트라(Della Pietra)에 의해 1996년 처음 도입됐다. 최대 엔트로피(Entropy)는 nltk.classify.maxent 모듈에서 NLTK에 정의된다. 이 모듈에서는, 모든 확률 분포(probability distributions)는 학습 데이터에 따른다고 생각된다. 이 모델은 두 가지 기능, 즉 입력 기능 및 관절 기능(input-feature and joint feature)을 참조하는 데 사용된다. 입력 기능은 라벨이 없는 단어의 특징이라고 할 수 있다. 결합된 기능(joined feature)은 라벨된 단어의 기능을 호출할 수 있다. MLE는 텍스트에서 특정 사건에 대한 확률 분포를 포함하는 freqdist를 생성하는데 사용된다. param freqdist은 확률 분포의 기반이 되는 빈도수 분포로 구성된다.

NLTK에서 최대 엔트로피 모델에 대한 코드를 확인할 것이다.

<소스>

2016.08.24

Page 28.

<소스>

이전의 코드에서, nltk.probability는 텍스트에서 개별 토큰의 발생 빈도수를 결정하기 위해 사용되는 FreqDist 클래스로 구성된다.

ProbDistI은 텍스트에서 개체 발생의 확률 분포를 결정하는 데 사용된다. 기본적으로 확률 분포의 두 종류가 있다. 유래된 확률 분포(Derived Probability Distribution)와 분석적인 확률 분포(Analytic Probability Distribution) 분산 확률 분포(Distributed Probability Distributions?)는 도수 분포(frequency distribution)에서 얻을 수 있다. 분석 확률 분포는 변화(variance)와 같은 파라미터(?)에서 얻을 수 있다.

빈도 분포를 얻기 위해, 최대 가능 추정치가 사용된다. 도수 분포에서 빈도수에 기초하여 각 발생 확률을 계산한다.

<소스>

확률 분포에 기초하여 상기 빈도수 분포를 발견할 것이다.

<소스>

Page 29.

<소스>

빈도수 분포를 얻기 위해 사용된다. 그 범위는 0과 1 사이에 있는 실수(real number), 감마(Gamma)에 의해서 표현된다. LidstoneProbDist는 다음과 같이 결과 N, 카운트 c와 bins B로 주어진 관찰의 확률을 계산한다. (c+Gamma)/(N+B*Gamma)

또한 감마(Gamma)는 각 bin의 카운트에 추가되고 MLE는 주어진 빈도수 분포로부터 계산되는 것을 의미한다.

<소스>

Lidstone은 freqdist을 얻기 위해 확률 분포를 계산하는 데 사용된다. paramfreqdist는 확률 추정치를 기초로 하는 빈도수 분포로 정의될 수 있다.

param 빈(bins)은 확률 분포로부터 얻어질 수 있는 샘플 값으로 정의될 수 있다. 확률의 합계는 다음과 동일하다.

<소스>

Page 30.

<소스>

확률 분포에 기초하는 빈도수 분포를 얻는다.

<소스>

Page 31.

<소스>

빈도수 분포를 얻기 위해 사용된다. 또한 카운트(count) c, 결과 N, bins B와 같이 샘플의 확률을 계산한다.

(c+1)/(N+B)

2016.08.26

또한, 1은 매 빈의 수에 추가되는 것을 의미하며,  최대 우도(maximum likelihood)는 결과 빈도수 분포로 추정된다. 

<소스>

LaplaceProbDist는 freqdist를 발생하는 확률 분포를 획득하기 위해 사용된다.

param freqdist는 확률 추정에 기초하여 도수 분포를 얻기 위해 사용된다.

Param 빈들은 생성할 수 있는 샘플 값의 빈도로 정의될 수 있다.
확률의 합은 1이어야 한다.

<소스>

Page 32.

도수 분포를 얻기 위해 사용된다. 또한 계수(count) C, 결과 N, 빈 B와 같은 샘플의 확률을 계산한다.

2016.08.29

WittenBellProbDist는 확률 분포를 획득하기 위해 사용된다. 이전에 본 샘플의 빈도수에 기초하여, 균일한 확률 질량(mass)을 얻기 위해 사용된다. 다음과 같은 샘플의 확률 질량이 주어진다. 샘플의 확률 질량은 다음과 같다.

T / (N + T)

여기서, T는 관측된 샘플의 수이고, N은 관측된 이벤트들의 총 수이다. 발생하는 새로운 샘플의 최대 공산 추정과 같다. 모든 확률의 총합은 1과 동일하다.

<소스>

Page 33.

<소스>

확률 분포를 획득한다. 이 확률은 보이지 않는 샘플에 균일한 확률 질량을 제공하는 데 사용된다. 샘플의 확률 질량은 다음과 같다.

T / (N + T)

여기서, T는 관측된 샘플의 수이고, N은 관측된 이벤트들의 총 수이다. 발생하는 새로운 샘플의 최대 공산 추정과 같다. 모든 확률의 총합은 1과 동일하다.

<소스>

Z는 이 값들과 빈 값을 이용하여 산출되는 정규화 요소이다.

Param freqdist는 확률 분포를 얻을 수 있는 빈도 수를 추정하기 위해 사용된다.

Param 빈은 샘플의 가능한 유형의 숫자로 정의될 수 있다.

<소스>

Page 34.

<소스>

최대 공산 추정을 이용하여 테스트를 수행할 수 있다. NLTK에서 MLE에 대한 다음 코드를 살펴 보자.

<소스>

Page 35.

은닉 마르코프 모델(Hidden Markov Model) 추정

은닉 마르코프 모델(HMM, Hidden Markov Model)은 관찰 상태와 이를 결정하는 데 도움이 되는 잠재 상태로 구성되어 있다. HMM의 도표 설명을 고려한다. 여기서, x는 잠재적인 상태를 나타내고, y는 관찰된 상태를 나타낸다.

<그림>

2016.08.30

좋은 튜링

좋은 튜링은 자신의 통계 조수 I.J. Good과 함께 앨런 튜링(Alan Turing)에 의해 도입됐다. 그것은 word sense disambiguation(WSD), amed entity recognition(NER), 맞춤법 교정, 기계 번역등 언어적 작업에서 수행되는 통계적 기법의 성능을 향상시키는 효율적인 스무딩 방법이다. 이 방법은 오브젝트의 보이지 않는 확률을 예측하는 것을 돕는다. 이 방법에서, 이항 분포(binomial distribution)는 관심 오브젝트에 의해 전시된다. 이 방법은 높은 카운트의 샘플에 기초하여 제로 또는 낮은 카운트 샘플의 질량 확률을 계산하기 위해 사용된다. 간단한 좋은 튜링은 로그 공간에서 직선(linear line)으로 선형 회귀(linear regression)에 의한 빈도수에서 빈도수까지 근사치를 수행한다. C\가 조정된 계수(count)이면, 다음을 계산할 것이다.

c\ = (c + 1) N(c + 1) / N(c)   for c >= 1

Page 38.

c == 0을 위한 훈련에서 제로 빈도수 샘플 = N (1). => 확인해야함

여기서, C는 원래의 수이고, N(i)는 count i로 관찰된 이벤트 타입의 수이다.

빌 게일(Bill Gale)과 제프리 샘슨(Geoffrey Sampson)은 간단한 좋은 튜링을 제시했다.

<소스>

Page 39.

소스

Page 40.

<소스>

재정규화(Renormalization)는 확률의 적절한 분포가 얻어지는 것이 매우 중요하다. 그것은 보이지 않는 샘플 N(1)/N의 확률 추정치를 결정하고, 이전에 눈에 보이는 모든 샘플 확률을 재정규화하여 얻을 수 있다.

Page 41.

<소스>

Page 42.

<소스>

NLTK에서 간단한 좋은 튜링의 코드를 살펴보자.

<소스>

Page 43.

Kneser Ney 추정

Kneser Ney는 트라이그램(trigrams)에서 사용된다. Kneser Ney 추정을 위한 NLTK에서 다음 코드를 살펴 보자.

<소스>

Witten Bell 추정

위튼 벨(Witten Bell)은 제로 확률을 갖는 알려지지 않은 단어를 처리하도록 설계된 스무딩 알고리즘이다. NLTK에서 위튼 벨 추정을 위한 다음 코드를 살펴 보자.

<소스>

Page 44.

MLE를 위한 백 오프 메커니즘(back-off mechanism) 개발

카츠 백 오프(Katz back-off)는 N 그램에서 이전 내용이 주어진 토큰의 조건부 확률을 계산하는 생성력이 있는 n 그램 언어 모델로 정의될 수 있다. 이 모델에 따르면, 훈련에서, n 그램이 n 배 이상 본다면, 이 후 이전의 정보를 제공하는 토큰의 조건부 확률은 n 그램의 MLE에 비례한다. 그렇지 않으면, 조건부 확률은 (n-1) 그램의 백 오프 조건부 확률에 해당한다.

다음은 NLTK에서 카츠의 백 오프 모델에 대한 코드이다.

<소스>

2016.09.02

 믹스 앤 매치를 얻기 위한 데이터 보간 적용(확인필요)
 
드문 텍스트를 처리 할 때 additive smoothed bigram 사용의 제한은 무지(ignorance)의 상태로 back off한다. 예를 들어, 매혹적인 단어는 훈련 데이터에서 다섯 번 발생한다.:by에 의한 세 번 the에 의한 두번

2016.09.19

Page 91.

CFG 규칙은 두 가지 유형이다. - 구문 구조 규칙과 문장 구조 규칙

구문 구조 규칙은 다음과 같이 정의될 수 있다.-A  Î N과 터미널과 비 터미널로 구성되는 A→a(확인) 

CFG의 문장 수준 건설(Sentence level Construction)에서, 네 개의 구조가 있다.

-Declarative structure: 선언적 문장으로 처리(주어가 술어 뒤에 따른다).(확인)

-Imperative structure: 필수 문장, 커맨드 혹은 제안 처리(문장은 동사구로 시작하고 제목을 포함하지 않는다.)

Page 92.

-Yes-No structure: 질문 - 응답 문장을 다룬다. 이러한 질문에 대한 답변은 yes 또는 no 이다.

-Wh-question structure: 질문 - 응답 문장을 다룬다.  Wh 단어로 시작하는 질문(누가, 무엇을, 어떻게, 언제, 어디서, 왜, 그리고 Which).

일반 CFG 규칙은 여기에 요약된다.

<예제>

NLTK에서 문맥 자유 문법 규칙(Context-free Grammar rules)의 사용을 도시한 예제를 살펴본다.

<소스>

Page 93.

<소스>

NLTK에서 ATIS 문법을 접속하는 예제는 다음과 같다.

<소스>

다음과 같이 ATIS에서 테스트 문장을 추출한다.

<소스>

바툼업 파싱

<소스>

Page 94.

<소스>

상향식(bottom-up), 왼쪽 코너(Left Corner) 파싱

<소스>

상향식 필터와 함께 왼쪽 코너 파싱(Left Corner parsing with a Bottom-up filter)

<소스>

하향식 파싱(Top-down  parsing)

<소스>

Page 95.

<소스>

증분 상향식 파싱(Incremental Bottom-up parsing)

<소스>

증분 상향식, 왼쪽 코너 파싱(Incremental Bottom-up, Left Corner parsing)

<소스>

Page 96.

상향식 필터와 증분 왼쪽 코너 파싱(Incremental Left Corner parsing with a Bottom-up filter)

<소스>

증분 하향식 파싱(Incremental Top-down parsing)

<소스>

Earley 파싱(Earley parsing)

<소스>

Page 97.

CFG에서 확률 문맥 자유 문법( 확률 문맥 자유 문법) 생성

Probabilistic Context-free Grammar(PCFG)에서, 확률은 CFG에 있는 모든 생산 규칙(production rules)에 부착된다. 이러한 확률의 합은 1이다. 그것은 CFG 같은 파스 구조를 생성하지만, 각 파스 트리에 확률을 할당한다. 파스 트리 확률은 트리를 구축하는데 사용되는 모든 생산 규칙 확률의 곱을 취함으로 얻어진다.

PCFG의 규칙의 형성을 설명한 NLTK의 다음 코드를 살펴보자.

<소스>

Page 98.

확률 차트 파싱(Probabilistic Chart Parsing)을 보여주는 NLTK의 코드를 살펴보자.

<소스>

재귀 하강 파싱(Recursive Descent Parsing) 단점은 좌 재귀 문제(Left Recursion Problem)를 일으키고 매우 복잡하다는 것이다. 그래서, CYK 차트 파싱이 도입됐다. 그것은 동적 프로그래밍 접근 방식을 사용한다. CYK는 간단한 차트 파싱 알고리즘 중 하나이다. CYK 알고리즘은 O(n3) 시간에서 차트를 구성할 수 있다. CYK 및 Earley 모두는 상향식 차트 파싱 알고리즘이다. 그러나 잘못된 파스가 구축될 때 Earley 알고리즘은 하향식 예측(Top-down predictions)을 사용한다.

Page 99.

CYK 파싱의 다음 예제를 살펴본다.

<소스>

초기화 테이블을 구성하는 다음 코드를 살펴본다.

테이블에서 가득차게 하는 다음 코드를 살펴본다.(확인)

<소스>

Page 100.

다음은 디스플레이 테이블을 구성하기위한 파이썬 코드이다.

<소스>

다음 코드는 결과를 보여준다.

<소스>

Earley 차트 파싱 알고리즘

Earley 알고리즘은 1970년 Earley에 의해 주어졌다.(확인) 이 알고리즘은 하향식 파싱(Top-down parsing)과 유사하다. 그것은 좌측 재귀(left-recursion)를 처리할 수 있으며 CNF를 필요로하지 않는다. 그것은 오른쪽 태도(right manner)에 왼쪽에 차트를 채운다.(확인)

Earley 차트 파서를 사용한 파싱을 보여주는 예제를 살펴본다.

<소스>

Page 101.

<소스>

NLTK에서 차트 파서를 사용한 파싱을 보여주는 예제를 살펴본다.

<소스>

Page 102.

NLTK에서 스테핑 차트 파서(Stepping Chart parser)를 사용한 파싱을 보여주는 예제를 살펴본다.

<소스>

Page 103.

<소스>

Page 104.

NLTK의 기능 차트 파싱(Feature chart parsing )의 코드를 살펴보자.

<소스>

Page 105.

<소스>

다음 코드는 Earley 알고리즘의 구현을 위한 NLTK에서 발견된다.

<소스>

Page 106.

<소스>

요약

5장에서 Treebank 코퍼스를 접속하고, 문맥이 없는 문법(Context-free Grammar), 확률 문맥 자유 문법(Probabilistic Context-free Grammar), CYK 알고리즘 및 Earley 알고리즘의 구현, 파싱에 대해 다뤘다. 따라서, 5장에서, NLP의 구문 분석 단계에 대해 살펴봤다.

다음 장에서 NLP의 또 다른 단계인 의미 분석에 대해 살펴볼 것이다. 다른 접근 방식을 사용하여 NER에 대해 살펴보고 명료화 작업(disambiguation tasks)을 수행하는 방법을 얻을 것이다.







