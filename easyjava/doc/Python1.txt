1장.

자연 언어 처리(NLP)는 자연 언어와 컴퓨터 간의 상호작용에 관한 것이다. 자연 언어 처리는 인공 지능(AI)과 전산 언어학의 주요 구성 요소 중 하나이다. 그것은 컴퓨터와 인간 사이의 원활한 상호작용을 제공하고 컴퓨터가 머신 러닝의 도움으로 인간의 언어를 이해할 수 있는 기능을 제공한다. 프로그래밍 언어 파일의 내용 혹은 문서를 나타내는 데 사용되는 기본 데이터 타입(예를 들면, C, C ++, JAVA, Python 등)을 문자열로서 알려졌다. 이 장에서는 다양한 NLP 작업을 수행하는 것이 유용한 문자열에서 수행할 수 있는 다양한 작업을 살펴볼 것이다.

이 장에서는 다음 내용을 포함할 것이다.

-텍스트의 토큰
-텍스트의 정규화
-토큰의 교체 및 수정
-지프(Zipf)의 법칙을 텍스트에 적용
-Edit Distance 알고리즘을 사용한 유사도 적용(?)
-Jaccard의 Coefficient를 이용한 유사도 적용(?)
-스미스 워터맨(Smith Waterman)를 이용한 유사도 적용(?)

토큰화

토큰화는 토큰이라는 작은 부분으로 텍스트를 분할하는 과정으로 정의될 수 있으며, NLP에서는 중요한 단계로 간주된다.

2016.08.08

Page 2.

NLTK가 설치되고 파이썬 IDLE가 실행되면, 각각의 문장에 텍스트 혹은 단락의 토큰을 수행할 수 있다. 토큰화를 수행하기 위해, 문장 토큰 함수를 임포트할 수 있다. 이 함수의 인자는 토큰화되야하는 텍스트일 것이다. sent_tokenize 함수는 PunktSentenceTokenizer로 알려진 NLTK의 인스턴스를 사용한다. NLTK 인스턴스는 이미 문장의 시작과 끝을 표시하는 문자, 문장 부호에 기초하여 다른 유럽 언어로 토큰화를 수행하도록 훈련되었다.

문장을 텍스트로 토큰화(확인)

이제, 주어진 텍스트를 개별 문장으로 토큰화하는 방법을 살펴보자.

<소스>

따라서, 주어진 텍스트는 각각의 문장으로 분할된다. 또한, 각각의 문장에 대한 처리를 수행할 수 있다.

여러 문장을 토큰화하기 위해, PunktSentenceTokenizer를 로드하고 토큰화를 수행하기 위해 tokenize() 함수를 사용할 수 있다.

2016.06.09

이제, 각각의 문장으로 처리를 수행할 수 있다. 개별 문장은 단어로 토큰화된다. 단어 토큰화는 word_tokenize() 함수를 사용하여 수행된다. word_tokenize 함수는 단어 토큰화를 수행하는 TreebankWordTokenizer으로 알려진 NLTK 인스턴스를 사용한다.

여기에서는 word_tokenize를 사용한 영어 텍스트의 토큰화를 보여준다.

<소스>

단어의 토큰화는 TreebankWordTokenizer 로드와 단어로 토큰화될 필요가 있는 문장인 그 인자에서 tokenize() 함수의 호출에 의해서 수행될 수 있다. 이 NLTK의 인스턴스는 이미 공간과 문장 부호에 기초를 위한 단어로 문장의 토큰을 수행하기 위해 훈련을 하고있다.

다음 코드는 사용자의 입력을 얻고 토큰화하고 그 길이를 평가하도록 도와준다.

<소스>

Page 4.

TreebankWordTokenizer를 사용한 토큰화

이제 TreebankWordTokenizer를 사용하여 토큰화를 수행하는 코드를 살펴보자.

<소스>

TreebankWordTokenizer는 Penn Treebank Corpus에 따라 규칙을 사용한다. 축소로 분리하여 작동한다.(확인) 다음과 같다.

<소스>

또 다른 tokenizer는 PunktWordTokenizer 이다. 그것은 분할한 구두점으로 작동한다. 각 단어는 완전히 새로운 토큰을 생성하는 대신 유지된다. 또 다른 단어 tokenizer는 WordPunctTokenizer 이다. 완전히 새로운 토큰을 구둣점으로 분할하여 제공한다. 이 분할 타입은 보통 바람직하다.

<소스>

tokenizers의 상속 트리는 여기에 주어진다.

<그림>

2016.08.10

Page 5.

정규식을 사용한 토큰화

단어의 토큰은 두 가지 방법의 정규식을 구성하여 수행될 수 있다. 

-단어로 매칭
-공백 혹은 간격으로 매칭

NLTK에서 RegexpTokenizer를 임포트할 수 있다. 텍스트에 존재하는 토큰을 일치할 수 있는 정규식을 생성할 수 있다.

<소스>

인스턴스하는 클래스 대신에, 토큰화의 다른 방법은 이 함수를 사용하는 것이다.

<소스>

RegularexpTokenizer는 토큰을 매칭시켜 토큰화를 수행하는 re.findall() 함수를 사용한다. 틈새(gaps) 혹은 공간(space)를 매칭시켜 토큰화를 수행하는 re.split() 함수를 사용한다.

여백(whitespace)을 사용하여 토큰화하는 방법의 예제를 살펴 보자.

<소스>

대문자로 시작하는 단어를 선택하기 위해, 다음과 같은 코드가 사용된다.

Page 6.

다음 코드는 미리 정의된 정규식이 RegexpTokenizer의 서브클래스에서 사용하는 방법을 보여준다.

<소스>

스트링의 토큰화는 공백-탭, 스페이스, 또는 줄 바꿈(newline)을 사용하여 수행할 수 있다.

<소스>

WordPunctTokenizersms는 알파벳과 알파벳이 아닌 문자로 텍스트의 토큰을 수행하기 위한 정규 표현식  \w+|[^\w\s]+ 을 사용한다.

split() 메소드를 사용한 토큰화는 다음 코드에서 확인된다.

<소스>

sent.split('\n'), LineTokenizer와 유사하게, 라인에 텍스트를 토큰화하여 작동한다.

<소스>

Page 7.

2016.08.11

nltk.tokenize.util.string_span_tokenize(sent,separator)는 분리 기호의 각 범위에 분할하여 전송되는 토큰의 오프셋을 반환할 것이다.

<소스>

Page 8.

정규화

자연어 텍스트 프로세싱을 수행하기 위하여, 주로 구두점 제거, 소문자 또는 대문자로 전체 텍스트를 변환, 단어로 번호 변환, 약어 텍스트 정규화 확대 등 정규화를 수행할 필요가 있다.

구두점 제거

때때로, 토큰화 동안, 구두점 제거하는 것이 바람직하다. 구두점의 제거는 NLTK에서 정규화하는 동안 주요 작업 중 하나로 간주된다.

다음 예제를 살펴본다.

<소스>

앞의 코드는 토큰화된 텍스트를 가져온다. 다음 코드는 토큰화된 텍스트에서 문장 부호(구두점?)를 제거할 것이다.

<소스>

Page 9.

소문자와 대문자로 변환

주어진 텍스트는 lower() 및 upper() 함수 사용으로 완전하게 소문자 혹은 대문자 텍스트로 변환될 수 있다. 대문자 혹은 소문자로 텍스트를 변환하는 작업은 정규화의 카테고리에 해당된다.

변환의 다음 예제를 살펴본다.

<소스>

정지 단어 처리

Page 13.

RepeatReplacer 클래스는 컴파일하는 정규 표현식 및 대체 문자열에 의해 작동하고 replacers.py에 존재하는 backreference.Repeat_regexp를 사용하여 정의된다. 그것은 0(zero) 혹은 많은(\w*)가 되는 시작하는 문자, 0(zero) 혹은 많은(\w*)가 되는 종료 문자 혹은 같은 문자를 따르는 문자(\w)로 매치된다.

예를 들어, lotttt는 (lo)(t)t(tt)로 분할된다. 여기에서, 하나의 t를 줄이면, 스트링은 lottt가 된다. 연속적인 분리 과정, 마지막으로 얻어진 결과 스트링은 lot이다.

happy를 hapy로 변환하는 것에 대한 RepeatReplacer의 문제는 부적절하다. 이 문제를 방지하기 위해, 그것과 함께 wordnet을 포함할 수 있다.

이전에 생성한 replacers.py 프로그램에서, wordnet을 포함하는 다음 라인을 추가한다.

<소스>

이제, 이전에 언급한 문제를 극복할 수 있는 방법을 살펴 보자.

<소스>

동의어로 단어 교체

이제 우리는 동의어로 주어진 단어를 대체할 수 있는 방법을 살펴볼 것이다.