About the Authors

Deepti Chopra는 Banasthali University의 조교수이다. 그녀의 주요 연구 분야는 전산 언어학, 자연어 처리 및 인공 지능이다. 그녀는 영어에서 인도어를 위한 MT 엔진 개발에도 참여한다.(확인) 그녀는 여러 저널 및 컨퍼런스에 다양한 간행물이 있으며 여러 컨퍼런스 및 저널의 프로그램 위원회에서도 활동한다.

Nisheeth Joshi는 Banasthali University의 부교수로 근무한다. 그의 관심 분야는 전산 언어학, 자연 언어 처리 및 인공 지능이다. 그는 TDIL 프로그램, 정보 기술부(Department of Information Technology), Govt로 empaneled 전문가 중 하나이다. 인도의, 인도에서 언어 기술 기금과 연구(Language Technology Funding and Research)를 총괄하는 최고의 기관. 그는 다양한 저널 및 컨퍼런스의 여러 간행물을 보유하고 있으며 여러 컨퍼런스 및 저널의 프로그램위원회 및 편집 위원으로도 활동한다. 

Iti Mathur는 Banasthali University의 조교수이다. 그녀의 관심 분야는 전산 의미론 및 존재론적 엔지니어링이다. 이 외에도, 영어에서 인도어까지 MT 엔진 개발에 참여한다. 그녀는 TDIL 프로그램, 전자 정보 기술 부서(Department of Electronics and Information Technology)(DeitY), 인도의 언어 기술 기금과 연구를 총괄하는 최고의 기관인 인도의 Govt empaneled 전문가 중 하나이다. 그녀는 여러 학술지와 학술회의에 여러 간행물을 보유하고 있으며 여러 회의 및 저널의 프로그램위원회 및 편집위원회에서 활동한다.

우리는 자연 언어 처리 기반 책을 출판하려는 목표를 달성하기 위해 전해진 축복에 대해 감사의 말을 전하고 모든 친지와 친척에게 진심으로 감사하다.

About the Reviewer

Arturo Argueta는 현재 고성능 컴퓨팅 및 NLP 연구를 수행하는 박사이다. Arturo는 클러스터링 알고리즘, NLP에 대한 기계 학습 알고리즘 및 기계 번역에 대한 연구를 수행했다. 그는 영어, 독일어 및 스페인어에도 능숙하다.

Preface

이 책에서는 Python에서 NLP의 다양한 작업을 구현하는 방법과 NLP의 최신 연구 주제에 대한 통찰력을 얻을 것이다. 이 책은 학생과 연구원이 실제 애플리케이션을 기반으로 자신의 프로젝트를 생성하는 데 도움이되는 포괄적인 단계별 안내서이다.

What this book covers

1장, '문자열 작업'은 토큰화 및 정규화와 같은 텍스트에 대한 사전 처리 작업을 수행하는 방법과 다양한 문자열 매칭 방법을 설명한다.

2장, '통계 언어 모델링'에서는 단어 빈도를 계산하고 다양한 언어 모델링 기법을 수행하는 방법을 다룬다.

3장, '형태학 - Getting Our Feet Wet'에서는 스테머, 형태소 분석기 및 형태소 생성기를 개발하는 방법을 다룬다.

4장, '품사 태깅 - 단어 식별'에서는 품사 태깅 및 n-gram 접근을 포함한 통계적 모델링을 다룬다.

5장, '파싱 - 트레이닝 데이터 분석'에서는 트리 뱅크 구성, CFG 구성, CYK 알고리즘, 차트 구문 분석 알고리즘 및 음역의 개념에 대한 정보를 살펴본다.

6장, '의미 분석 - Meaning Matters'에서는 Shallow Semantic Analysis(즉, NER)과 WordNet을 사용하는 WSD의 개념과 애플리케이션에 대해 살펴본다.

7장, 'Sentiment Analysis - I Am Happy'에서는 감정 분석의 개념을 이해하고 적용하는 데 도움이 되는 정보를 제공한다.

8장, '정보 검색 - 정보 액세스'는 정보 검색 및 텍스트 요약의 개념을 이해하고 적용하는 데 도움이될 것이다.

9장, '담화 분석 - 아는 것은 믿는 것', 담화 분석 시스템과 해결 기반 시스템을 개발한다.

10장, 'NLP 시스템 평가 - 성능 분석'에서는 NLP 시스템을 평가하는 개념을 이해하고 적용하는 방법에 대해 살펴본다.

What you need for this book

모든 장에서 Python 2.7 혹은 3.2+가 사용된다. NLTK 3.0은 32 비트 머신 혹은 64 비트 머신에 설치해야한다. 필요한 운영 체제는 Windows/Mac/Unix이다.

Who this book is for

이 책은 합리적인 지식 수준과 Python에 대한 이해를 바탕으로 NLP의 중급 개발자를 대상으로 한다.

Conventions

이 책에서는 여러 종류의 정보를 구별하는 다양한 텍스트 스타일을 확인할 수 있다. 다음은 이러한 스타일의 예제와 그 의미에 대한 설명이다.

텍스트에서 코드 단어, 데이터베이스 테이블 이름, 폴더 이름, 파일 이름, 파일 확장자, 경로 이름, 더미 URL, 사용자 입력 및 Twitter 핸들은 다음과 같다.

"For tokenization of French text, we will use the french.pickle file."

코드 블록은 다음과 같이 설정된다.

<소스>

이와 같은 상자에 경고 혹은 중요한 메모가 나타난다.

팁과 트릭은 이와 같이 나타난다.

Page 107.

의미분석,
=>의미분석

문자 시퀀스
=>의미 분석은 문자 시퀀스

정의된다.
=>정의한다.

사용될 수 있다.
=>사용할 수 있다.

Page 108.

자연어
=> 자연 언어

실행할 수 있는 계산을
=> 계산을 실행하는 것을

수행된다.
=> 수행될 것이다.

의미분석 해석
=> 의미 해석

문장에서 의미를
=>의미를 문장에

콘텐츠 해석 ~ 매핑한다.
=>문맥 해석은 논리 형식을 지식 표현으로 매핑하는 것이다.

의미론적 분석의
=> 의미 분석의

나타낼 수 있다.
=>표현할 수 있다.

도움을 나타낼 수 있다.
=>도움으로 그것을 나타낼 수 있다.

개념에게 방법을 주었다.
=>스크립트의 개념으로 나아 갔다.

Page 121.

정의될 수 있다.
=> 정의할 수 있다.

응답은 ~ 경우
=> response가 answer key와 정확히 일치하면

응답은 ~ 않으면
=>response가 answer key와 같지 않은 경우

응답 키는 ~ 않은 경우
=>answer key는 태그가 되었지만 response가 태그되지 않은 경우

응답이 ~ 않는다.
=>response가 태그 지정되었지만 answer key는 태그가 지정되지 않은 경우

기계 학습 ~ NER
=>기계 학습 툴킷을 사용한 NER 훈련

손수
=>수작업

그것은 ~ 증명한다.
=>기계 학습 기반 접근법이 규칙 기반 접근법을 능가한다는 것이 실험적으로 입증됐다.

규칙 ~ 것이다.
=> 또한 규칙 기반 접근 방식과 기계 학습 기반 접근 방식을 함께 사용하면 NER 성능이 향상된다.

Page 122.

사용할 수있는
=>사용할 수 있는

Page 123.

지명된 ~ 식별할 수 있다.
=> 명명된 엔티티를 식별할 수 있다.

지명된 엔티티
=>Named Entities

Page 127.

모호성은 ~ 작업이다.(확인)

disambiguation => 중의성 제거

명료화 => 중의성 제거

작업의 => 타스크의

로우 카운트를 ~ 사용=>원시 계수를 사용하는 대신에 코사인을 사용하여 중복을 계산

Wordnet의 의미 유사성(확인)

Page 128.

2개의 ~ 반환된다.=>두 개의 토큰을 비교할 때 두 토큰의 유사성을 결정하는 점수(최소 공통 하위 집합)가 반환된다.

두 개의 ~ 정의한다.=>두 감각의 깊이에 기초하여 두 토큰 사이의 유사성과 최소 공통 하위 사용자(Least Common Subsumer)를 정의한다.

분류에=>분류법에

가장 짧은 경로=> 최단 경로

유사도 점수=> 유사성 점수

2개 입력=>2개의 입력

두 입력=>2개의 입력

내용 정보=>콘텐츠 정보

경로 ~ 살펴보자.=>path similarity를 나타내는 NLTK의 다음 예제를 살펴보자.

leacock ~ 살펴보자.=> Leacock Chodorow Similarity을 묘사하는 NLTK의 다음 예제를 살펴보자.

Page 129.

기반으로=>기반한

NLTK에=>NLTK의

Page 130.

명확화 작업 ~ 살펴본다.=> disambiguation 작업을 수행하는 데 사용되는 NLTK의 Lesk 알고리즘에 대한 다음 코드를 살펴본다.

Page 131.

Wordnet과 ~ 논의했다.=> NER, HMM을 사용한 NER, Machine Learning Toolkits를 사용한 NER, NER의 성능 메트릭, POS 태깅을 사용하는 NER, Wordnet을 사용한 WSD 및 Synsets 생성에 대해 살펴봤다.

9장=>7장

Page 124.

Iterate over all synsets with a given part of speech tag.=>주어진 부분의 음성 태그로 모든 synsets를 반복한다. pos를 지정하지 않으면 모든 품사의 모든 synsets이 로드된다.

#각 품사에 대한 모든 synsets 생성
#파일을 열어서 읽는다. 여기에서 self._data_file_map의 파일 poitners는 다시 사용할 수 없다는 것을 기억한다. iterator를 정의하고 있기 때문에 우리가 보지 않는 동안 파일 포인터가 이동될 수 있다.

#POS 파일의 각 행에 대한 synsets 생성

#synset이 캐싱되는지 확인한다.

#그렇지 않으면 라인을 파싱한다.

#형용사 위성은 형용사와 같은 파일에 있으므로 실제로 위성인 경우 synset 만 생성한다.(확인)
#다른 모든 POS 태그의 경우 모든 synsets를 얻는다(이것은 형용사가 형용사 위성을 포함한다는 것을 의미한다).(확인)
#오픈한 여분의 파일 핸들을 닫는다.

'''두 단어의 단어 감각 사이의 유사성을 찾는다.'''
