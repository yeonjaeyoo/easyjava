7장. 감정 분석 - I Am Happy

감정 분석 혹은 감정 생성은 NLP의 작업 중 하나이다. 그것은 문자 시퀀스 뒤에 감정을 결정하는 과정으로 정의된다. 그것은 스피커(speaker) 혹은 사람이 본문그대로 행복하거나 슬픈 분위기에 있는지 여부를 확인하는 데, 또는 중간 표현을 나타낼 수 있다.(확인)

7장에서는 다음 내용을 살펴본다.

-감정 분석 소개
-NER를 사용한 감정 분석
-기계 학습을 이용한 감정 분석
-NER 시스템의 평가

Page 134.

감정 분석 소개

감정 분석은 자연 언어에서 수행되는 작업으로 정의할 수 있다. 여기에서, 계산은 긍정적, 부정적 혹은 중립적인 감정의 표현 여부를 결정하기 위해 자연 언어로 표현된 문장 혹은 단어에서 수행된다. 표현되는 텍스트에 대한 정보를 제공하기 때문에 감정 분석은 주관적인 작업이다. 감정 분석은  두 종류의 분류 문제로 정의될 수 있다.-이진 분류(긍정적 혹은 부정적) 및 멀티 클래스 분류(긍정적, 부정적 혹은 중립적) 감정 분석은 텍스트 감정 분석이라 한다. 텍스트 뒤에 감정(sentiments) 혹은 감정(emotions)을 결정하는 텍스트 마이닝 접근 방법이다. 주제 마이닝과 감정 분석을 결합 할 때, 주제-감정 분석이라 한다. 감정 분석은 사전(lexicon)을 사용하여 수행될 수 있다. 사전은 특정 도메인 혹은 일반 목적의 성격이될 수 있다. 사전은 긍정적 표현, 부정적 표현, 중립식 표현과 불용어 목록을 포함할지도 모른다. 테스트 문장이 나타날 때, 간단한 조회 작업은 사전을 통해 수행할 수 있다.

2016.10.05

단어 목록에 대한 하나의 예제 - Affective Norms for English Words (ANEW). 그것은 플로리다 대학(the University of Florida)에서 발견되는 영어 단어의 목록이다. 그것은 지배, 원자가 및 각성을 위한 1034 단어로 구성된다. 그것은 브래들리(Bradley)와 랭(Lang)에 의해 형성됐다. 이 단어 목록은 연구 목적(research purposes)이 아니라 학술 목적(academic purposes)으로 작성됐다. 다른 변종은 DANEW(네덜란드 ANEW) 및 SSPANEW(스페인어 ANEW) 이다.

AFINN은 2477 단어로 구성된다(이전 1468 단어). 이 단어 목록은 Finn Arup Nielson에 의해 형성됐다. 이 단어 목록을 작성하는 주요 목적은 트위터 텍스트에 대한 심리 분석을 수행하는 것이다. -5부터 +5까지 범위의 가수 값(valence value)은 각 워드에 할당된다.

Balance Affective 단어 목록은 277 영어 단어로 구성된다. 원자가 코드는 1에서 4의 범위이다. 1은 긍정적을 의미하고, 2는 부정적을 의미하고, 3은 걱정을 의미하고, 4는 중립을 의미한다.

Berlin Affective Word List(BAWL)는 독일어로 2,200 단어로 구성된다. BAWL의 또 다른 버전은 단어에 대한 추가 각성으로 구성된 Berlin Affective Word List Reloaded(BAWL-R)이다.

Bilingual Finnish Affective Norms는 210 영국 영어 뿐만아니라 핀란드어 명사를 포함한다. 금기 단어도 포함한다.

Compass DeRose Guide to Emotion Words는 영어에서 감정적인 단어로 구성된다. 이것은 Steve J. DeRose에 의해 형성됐다. 단어는 구분되었으나, 더 이상 가수와 각성은 없다.

Page 135.

Dictionary of Affect in Language(DAL)은 감정 분석에 사용할 수 있는 감정적인 단어를 포함한다. 그것은 Cynthia M. Whissell에 의해 형성됐다. 그래서 그것은 또한 Whissell's Dictionary of Affect in Language(WDAL)로 지칭된다.

General Inquirer는 많은 사전으로 구성된다. 이것에서는, 긍정 목록은 1915 단어를 포함하고, 부정 목록은 2291 단어를 포함한다.

Hu-Liu opinion Lexicon(HL)은 6800 단어의 목록을 포함한다(긍정적 및 부정적).

Leipzig Affective Norms for German(LANG)는 독일에서 1000 명사로 구성된 목록이며, 원자가, 구체성과 각성을 기반으로 등급을 가진다.

Loughran and McDonald Financial Sentiment Dictionaries은 Tim Loughran과 Bill McDonald에 의해 생성됐다. 이 사전은 긍정, 부정, 혹은 모달 단어(modal words)인 금융 문서에 대한 단어로 구성된다.

무어는 지배, 각성 및 원자가 관련 네덜란드어의 단어 목록으로 구성된다. 

NRC Emotion Lexicon는  Saif M. Mohammad에 의해 Amazon Mechanical Turk를 통해 개발된 단어의 목록으로 구성된다.

OpinionFinder의 Subjectivity Lexicon은 8221 단어 목록을 포함한다(긍정적 혹은 부정적).

SentiSense는 14 감정 카테고리에 따라 2190 synset과 5496 단어를 포함한다. Warringer는 지배, 각성 그리고 원자가 관련된 Amazon Mechanical Turk에서 수집된 영어로 13,915 단어를 포함한다.

labMT는 10,000 단어로 구성된 단어 목록이다.

영화 리뷰에 대한 감정 분석을 수행하는 NLTK에서 다음 예제를 살펴보자.

<소스>

Page 136.

<소스>

여기서, 그것은 정보의 기능이 문서에서 존재하지 여부를 체크한다.

의미론적 분석의 또 다른 예제를 살펴보자. 먼저, 텍스트의 전처리를 수행한다. 이러한 점에서, 개별 문장은 주어진 텍스트로 식별된다. 이 후, 토큰은 문장에서 식별된다. 각 토큰은  세 개의 엔티티, 즉, 단어, 표제어(lemma) 및 태그를 포함한다.

Page 137.

텍스트의 전처리에 대한 NLTK의 다음 코드를 살펴보자.

<소스>

생성된 표제어는 워드 형태로 동일하다. 태그는 POS 태그이다. 단어, lemma 및 POS 태그인 각 토큰에 대한 세 개의 튜플을 생성하는 다음 코드를 고려한다.

<소스>

긍정적 및 부정적 표현으로 이루어진 두 종류의 사전을 생성할 수 있다. 사전을 사용하여 처리되는 텍스트에 태그를 수행할 수 있다.

Page 138.

사전을 사용한 태그에 대해 다음 NLTK 코드를 살펴보자. 

<소스>

Page 139.

<소스>

여기서, 전처리된 텍스트의 단어는 사전의 도움으로 긍정적 혹은 부정적 같은 태그이다.

긍정적 표현과 부정적 표현의 수를 계산하는데 사용되는 NLTK에서 다음 코드를 살펴보자.

<소스>

nltk.sentiment.util 모듈은 Hu-Liu lexicon를 사용하여 감정 분석을 수행하는 NLTK에서 사용한다. 이 모듈은 어휘(lexicon)의 도움으로, 긍정적, 부정적 및 중립적 표현의 수를 카운트하고 이 후 텍스트가 긍정적, 부정적 혹은 중립적 감정으로 이루어져 있는지 다수 카운트에 기초하여 결정한다. 어휘에서 사용할 수 없는 단어는 중립적인 것으로 간주된다.

NER를 사용한 감정 분석

NER는 명명된 엔티티를 발견하고 다른 개체명 클래스로 명명된 개체를 분류하는 과정이다. NER은 규칙 기반 접근(Rule-based approach), List look up approach, and  Statistical approaches(Hidden Markov Model, Maximum Entropy Markov Model, Support Vector Machine, Conditional Random Fields, and Decision Trees) 같은 다양한 기술을 사용하여 수행할 수 있다.

Page 140.

명명된 개체는 리스트에서 식별되는 경우, 그 때 제거되거나 문장으로부터 필터될 수 있다. 마찬가지로, 불용어도 제거할 수 있다. 명명된 개체는 감정 분석에 기여하지 않는 단어이기 때문에 이제 심리 분석은 나머지 단어에 대해 수행될 수 있다.

2016.10.10

기계 학습을 이용하여 감정 분석

NLTK에서 nltk.sentiment.sentiment_analyzer 모듈은 감정 분석을 수행하기 위해 사용한다. 기계 학습 기술에 기초한다.

NLTK에서  nltk.sentiment.sentiment_analyzer 모듈의 다음 코드를 살펴보자.

<소스>

텍스트에서 모든 단어(중복)를 반환하는 다음 코드를 살펴본다.

<소스>

Page 141.

<소스>

텍스트에 특징 추출 기능(feature extraction function)을 적용하는 다음 코드를 살펴본다.

<소스>

단어의 기능을 반환하는 다음 코드를 살펴본다.

<소스>

다음 코드는 바이그램 기능을 반환한다.

<소스>

가능한 기능 세트를 사용하여 해당 인스턴스를 분류하기 위해 사용할 수 있는 다음 코드를 살펴보자.

<소스>

텍스트에서 기능 추출을 위해 사용할 수 있는 다음 코드를 살펴보자.

<소스>

Page 142.

훈련 파일에 대한 훈련을 수행하는 데 사용할 수 있는 다음 코드를 살펴보자. Save_classifier는 파일에 출력을 저장하는 데 사용된다.

<소스>

테스트 데이터를 이용하여 분류기의 테스트와 성능 평가를 수행할 수 있는 다음 코드를 살펴 보자.

<소스>

Page 143.

<소스>

트위터는 tweets라는 메시지를 생성하는데 사용되는 가장 인기있는 블로그 서비스의 하나로 간주될 수 있다. tweets는 긍정적 혹은 부정적 혹은 중립 감정과 관련된 단어를 포함한다.

감정 분석을 수행하기 위해, 나이브 베이즈 분류기, 최대 엔트로피 분류기, 서포트 벡터 머신 분류기 같은 기계 학습 분류기, 기계 학습 분류, 통계 분류기 혹은 자동 분류기를 사용할 수 있다.

분류를 위한 훈련 데이터를 필요로 하기 때문에 이 기계 학습 분류기 혹은 자동 분류기는 감독 분류를 수행하는 데 사용된다.

특징 추출에 대한 NLTK에 다음 코드를 살펴보자.

<소스>

Page 144.

<소스>

Page 145.

<소스>

분류기의 훈련 동안, 기계 학습 알고리즘에 입력은 레이블 및 특징이다. 입력이 특징 추출기(feature extractor)에 주어질 때 기능은 특징 추출기로부터 얻어진다. 예측 중에 라벨은 분류기 모델의 출력으로 제공되고 분류 모델의 입력은 특징 추출기를 사용하여 얻어진 기능이다. 같은 과정을 설명하는 다이어그램을 살펴보자.

<그림>

Page 146.

이제, 나이브 베이즈 분류기(Naive Bayes Classifier)를 사용하여 감정 분석을 수행하는 데 사용할 수있는 다음 코드를 살펴 보자.

<소스>

최대 엔트로피를 사용한 감정 분석의 다음 코드를 살펴 보자.

<소스>

NER 시스템의 평가

성능 측정 혹은 평가는 NER 시스템의 성능을 표시하는 데 도움이 된다. NER 태거의 결과는 response와 answer key로 인간의 해석으로 정의할 수 있다. 그래서, 다음과 같은 정의를 제공할 것이다.

Correct: 응답은 정확히 응답 키와 동일한 경우
Incorrect: 응답이 응답 키와 동일하지 않으면
Missing: 응답 키는 태그가 발견되지만 응답이 태그되지 않은 경우
Spurious: 응답이 태그를 발견하지만, 응답 키는 태그되지 않은 경우

NER 기반 시스템의 성능은 다음과 같은 파라미터를 이용하여 판단할 수 있다.

-Precision (P): P=Correct/(Correct+Incorrect+Missing)
-Recall (R): R=Correct/(Correct+Incorrect+Spurious)
-F-Measure: F-Measure = (2*P*R)/(P+R)

Page 147.

HMM을 사용한 NER의 코드를 살펴보자.

<소스>

Page 148.

<소스>

Page 149.

<소스>

Page 150 ~ 153.

<소스>

Page 154.

<소스>

HMM 사용한 NER에 의해 생성된 출력을 평가하는 데 사용되는 파이썬에서 다음 코드를 살펴보자.

Page 155 ~ 163

<소스>

Page 164.

파이썬에서 앞의 코드는 NER은 HMM을 사용하여 수행하는 방법과 NER 시스템은 성능 메트릭 (Precision, Recall, F-Measure)을 사용하여 평가하는 방법을 보여준다.

요약

7장에서는 NER 및 기계 학습 기술을 사용한 감정 분석을 살펴봤다. NER 기반 시스템의 평가도 다뤘다.

다음 장에서는 정보 검색, 텍스트 요약, 불용어 제거, 질의 응답 시스템 등을 살펴볼 것이다.

8장. 정보 검색 - 액세스 정보

정보 검색은 자연 언어 처리의 많은 애플리케이션 중 하나이다. 정보 검색은 사용자에 의해 생성된 쿼리에 대응하는 정보(예를 들어, Ganga가 문서에 나타난 단어의 수)를 검색하는 과정으로 정의할 수 있다.

8장에서는 다음 내용이 포함된다.

-정보 검색 소개
-불용어 제거
-벡터 공간 모델을 사용한 정보 검색

2016.10.11

-벡터 공간 스코링 및 쿼리 연산자 상호 작용
-잠재 의미론적 인덱싱을 이용한 IR 시스템 개발
-텍스트 요약
-질문-응답 시스템

정보 검색 소개

정보 검색은 사용자에 의해 이루어진 쿼리에 대한 응답으로 가장 적합한 정보를 검색하는 과정으로 정의할 수 있다. 정보 검색에서, 검색은 메타데이터 혹은 컨텐츠 기반의 인덱스에 기초하여 수행한다. 정보 검색의 하나의 예제는 각 사용자 질의에 대응하고 응답이 사용되는 정보 검색 알고리즘에 기초하여 제공되는 구글 검색이다. 정보 검색 알고리즘은 인덱싱 메커니즘을 사용한다. 인덱싱 메커니즘은 반전 지표로 알려져있다. IR 시스템은 정보 검색 작업을 수행하는 인덱스 postlist를 생성한다.

Page 166.

부울 검색은 관련 정보를 검색하기 위해 포스트 목록에 적용되는 부울 연산에서 정보 검색 작업이다.

정보 검색 작업의 정확성은 정도와 회수의 관점에서 측정된다.

쿼리가 종료될 때 주어진 IR 시스템은 X 문서를 반환한다고 가정한다. 그러나 반환해야 할 문서의 실제 혹은 골드 세트는 Y이다.

리콜은 시스템이 발견한 골드 문서의 비율로서 정의될 수 있다. 그것은 true positives와 false negatives의 조합에 true positives의 비율로 정의될 수 있다.

Recall (R) = ( X ∩  Y ) / Y

정밀도는 IR 시스템이 감지하고 올바르게한 문서의 비율로서 정의될 수 있다.

Precision (P) = ( X ∩  Y ) / X

F-측정은 정밀도와 리콜의 조화 평균으로 정의할 수 있다.

F-Measure = 2 * ( X ∩  Y ) / ( X + Y )

불용어 제거

정보 검색을 수행하는 동안, 문서에서 불용어를 검색하고 이를 제거하는 것은 중요하다.

이제 NLTK의 영어 텍스트에서 검출될 수 있는 불용어의 집합을 제공하기 위해 사용될 수 있는 다음 코드를 살펴 보자.

<소스>

Page 167.

<소스>

NLTK는 11개의 언어에서 2,400 불용어로 구성한 불용어 코퍼스(stop word corpus)로 구성된다.

단어를 중지하지 않는 텍스트에서 단어의 일부를 찾을 수 있는 NLTK의 다음 코드를 살펴보자.

<소스>

주어진 텍스트에서 불용어를 제거하는 데 사용할 수 있는 NLTK의 다음 코드를 살펴 보자. A같은 대문자에서 불용어는 소문자로 변환되고 이 후 제거하기 위해서 여기에, lower() 함수는 이전 불용어 제거에 사용된다.

<소스>

Page 168.

2016.10.12

<소스>

벡터 공간 모델을 사용한 정보 검색

벡터 공간 모델에서, 문서는 벡터로 표시한다. 벡터와 같은 문서를 표현하는 방법 중 하나는 TF-IDF(Term Frequency-Inverse Document Frequency)를 사용한다.

Term frequency may be defined as the total number of times a given token exists in a document divided by the total number of tokens.(확인) 또한, 그것은 주어진 문서의 특정 용어의 발생 빈도로 정의될 수 있다.

TF(term frequency)의 공식은 다음과 같다.

TF(t,d) = 0.5 + (0.5 * f(t,d)) / max {f(w,d) : w∊d}

IDF는 문서 빈도의 역으로 정의할 수 있다. 또한, 주어진 기간이 공존하는 코퍼스에 놓여진 문서 개수로 정의한다.

IDF는 특정 토큰이 존재하는 문서의 수로 나눈 특정 코퍼스에서 문서의 전체 개수의 로그를 구함으로써 계산할 수 있다.

IDF(t,d)의 공식은 다음과 같이 언급할 수 있다.

IDF(t,D)= log(N/{d∊D :t∊∊d})

TF-IDF 점수는 두 점수를 곱함으로 얻을 수 있다. 이것은 다음과 같이 쓰여진다.

TF-IDF(t, d, D) = TF(t,d) * IDF(t,D)

TF-IDF는 주어진 문서와 그것이 얼마나 코퍼스에 걸쳐 확산되고 존재함으로서 기간의 빈도의 추정치를 제공한다.

Page 169.

주어진 문서에 대한 TF-IDF를 계산하기 위해, 다음 단계가 필요하다.

-문서의 토큰화
-벡터 공간 모델의 계산
-각 문서에 대한 TF-IDF의 계산

토큰화의 처리는 우선 문장에 텍스트를 토큰화하는 것을 포함한다.