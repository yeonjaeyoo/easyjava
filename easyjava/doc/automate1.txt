1. 웹 작업

인터넷없는 삶을 상상해본 적이 있을까? 음식 주문에 대한 정보를 교환하는 것부터 거의 모든 것이 오늘날 인터넷에 크게 의존한다. 흥미로운 World Wide Web의 세계를 살펴보고 파이썬(Python) 모듈을 사용하여 상호 작용할 수 있는 다양한 방법을 살펴보자.

1장에서는 다음 내용을 살펴본다.

-HTTP 요청생성
-웹 스크래핑(web scraping)에 대한 간략한 설명
-웹 컨텐츠(web content) 파싱(Parsing) 및 추출(extracting)
-웹 컨텐츠 다운로드
-서드 파티(third-party) REST API 작업
-파이썬의 비동기 HTTP 서버
-셀레늄 바인딩(selenium bindings)을 사용한 웹 자동화
-웹 스크래핑과 리드 생성 자동화

소개

인터넷은 삶을 매우 쉽게 만들어주기 때문에 때로는 그 힘을 깨닫지 못한다. 친구 상태를 확인, 부모님 전화, 중요한 비즈니스 이메일(e-mail)에 응답하거나 게임을 하는 등- 오늘날 거의 모든 일에 대해 WWW(World Wide Web)를 사용한다.

Page 13.

이 레시피를 실행하려면 파이썬 v2.7을 설치해야한다. 일단 설치되면 파이썬 pip를 설치해야한다. PIP는 Pip Installs Packages의 약자로 컴퓨터에 필요한 파이썬 패키지를 다운로드하고 설치하는 데 사용할 수 있는 프로그램이다. 마지막으로 HTTP 요청을 하기 위해 requests 모듈이 필요하다.

requests 모듈을 설치하여 시작할 것이다(운영 체제를 기반으로 머신에서 수행할 수 있도록 파이썬 및 pip 설치를 남겨둘 것이다). 다른 전제 조건은 없다. 이제, 서둘러서 시작하자.

사용 방법

1. Linux/Mac 컴퓨터에서 터미널(Terminal)로 이동하여 다음 커맨드를 실행한다.

pip install -U requests

파이썬 사이트 패키지에 대한 권한이 없는 경우 sudo만 사용해야한다. 그렇지 않으면 sudo가 필요하지않다.(확인)

2. 다음 코드는 파이썬의 requests 모듈에서 HTTP GET 요청을하는 데 도움이 된다.

import requests r =
requests.get('http://ip.jsontest.com/')
print("Response object:", r)
print("Response Text:", r.text)

3. 다음 출력을 확인한다.

<그림>

Page 14.

4. 데이터 페이로드(data payload)로 HTTP GET 요청을 생성하는 것은 요청에서 간단하다. 다음 코드는 이를 달성하는 데 도움이 된다. 전송될 URL 요청을 확인할 수도 있다.

payload = {'q': 'chetan'} r =
requests.get('https://github.com/search', params=payload)
print("Request URL:", r.url)

<그림>

5. 이제 requests 모듈을 사용하여 HTTP POST 호출을 생성해보자. 이는 웹 사이트에 로그인 혹은 가입 양식을 채우고 포스트(POST) 하는 것과 유사하다.

payload = {'key1': 'value1'} r =
requests.post("http://httpbin.org/post", data=payload)
print("Response text:", r.json())

<그림>

Page 15.

오류(errors) 및 예외 처리(exceptions)는 요청에 대해서도 매우 편리하다. 다음 단편적인 코드는 오류 처리의 예제를 보여준다. 머신에서 인터넷 연결없이 코드를 실행하면 예외가 발생한다. 예외 처리기(exception handler)는 예외를 캐치(catch)하고 예상대로 새 연결을 설정하지 못했다.(확인)

try:
    r = requests.get("http://www.google.com/")
except requests.exceptions.RequestException as e:
    print("Error Response:", e.message)

어떻게 작동 하는가?

이 레시피에서는 파이썬의 requests 모듈을 사용하여 다양한 유형의 HTTP 요청을 생성하는 방법을 살펴본다. 이 코드가 어떻게 작동하는지 살펴보자.

- 첫 번째 예제에서는 http://ip.jsontest.com에 GET 요청을 전송하고 응답 코드와 응답 텍스트를 받았다. 인터넷에서 컴퓨터의 현재 IP 주소를 반환한다.
- 두 번째 예제에서는 페이로드 데이터로 HTTP GET 요청을 수행했다.    

2017.03.09

웹 스크래핑(web scraping) 살펴보기

웹 스크래핑을 수행하는 방법을 배우기 전에 스크래핑의 의미를 이해하자. 웹 세계에서 스크래핑은 컴퓨터 프로그램의 도움으로 상기 포맷으로 필요한 정보를 추출하려는 의도로 웹 사이트의 페이지를 가려내는 방법이다. 예를 들어 블로그에 게시된 모든 기사의 제목과 날짜를 가져 오려면 블로그를 통해 긁어 모으고 필요한 데이터를 가져오고, 요청에 기반이 되는 데이터베이스(database) 혹은 플랫 파일(flat file)에 저장하는 프로그램을 생성할 수 있다.

웹 스크래핑(Web scraping)은 종종 웹 크롤링(web crawling)과 혼동된다. 웹 크롤러(web crawler)는 웹 인덱싱의 목적으로 웹을 체계적으로 탐색하고 사용자가 웹을보다 효과적으로 검색할 수 있도록 웹 페이지 색인을 위한 검색 엔진에서 사용되는 봇이다.

그러나 스크래핑은 쉽지 않다. 우리에게 흥미로운 데이터는 XML 태그 혹은 HTML 태그에 포함된 특정 형식의 블로그 혹은 웹 사이트에서 사용할 수 있다. 따라서 필요한 데이터를 추출하기 전에 형식을 알아야 한다. 또한 웹 스크래퍼는 추출된 데이터를 나중에 처리하기 위해 저장해야하는 형식을 알아야한다. 브라우저 표시가 같을지라도 HTML 혹은 XML 형식이 변경되면 스크래핑 코드는 실패한다는 것을 이해하는 것도 중요하다.

2017.03.10

웹 스크래핑의 적법성

웹 스크래핑은 항상 법적인 측면에서 스캐너 아래에 있었다. 웹 스크래핑을 할 수 있을까? 법적 혹은 윤리적인 방법은 무엇인가? 수익 창출을 위해 스크래핑으로 확보한 데이터를 사용할 수 있을까?

이 주제는 많은 논의를 거쳤지만, 웹에서 저작권 정보를 스크랩하거나, 컴퓨터 사기 및 남용 방지법을 위반하거나, 웹 사이트의 서비스 약관을 위반하면 웹 스크래핑 관련 문제가 발생할 수 있다. 예를 들어, 공개 데이터를 가져 오기 위해 웹을 스크래핑하고 있다면 여전히 괜찮을 것이다. 그러나 그것은 매우 문맥적이며 사용자가 스크래핑하는 것과 데이터를 사용하는 방법에 대해 주의해야 한다.

Page 17.

다음은 데이터 스크래핑에 대한 웹의 몇 가지 지침이다.

-https://en.wikipedia.org/wiki/Web_scraping#Legal_issues
-https://www.quora.com/What-is-the-legality-of-web-scraping

Getting ready

https://github.com/ 웹 사이트에서 가격 데이터를 사용하여 파이썬으로 웹 스크래핑을 살펴본다. 이것은 매우 사소한 예제이지만 스크래핑 하는 데 걸리는 시간을 단축시킨다. 이 파이썬 레시피를 사용하여 흥미로운 데이터를 시작하고 스크랩하자.

How to do it

1. 컴퓨터에서 구글 크롬 브라우저를 실행시키고 https://github.com/pricing/ 웹 페이지를 연다. 이 페이지에서는 개인(Personal), 조직(Organization) 및 엔터프라이즈(Enterprise)와 같은 여러 가지 가격 정책을 확인할 수 있다.
2. 이제 브라우저에서 Personal 계획의 가격을 마우스 오른쪽 버튼으로 클릭하고 다음 스크린 샷처럼 Inspect 요소를 클릭한다.

<그림>

Page 18.

3. Inspect를 클릭하면 크롬 브라우저 콘솔 로그가 열리며 다음과 같은 GitHub 가격 페이지의 HTML 구조를 이해하는 데 도움이 된다. 

<그림>

4. 하이라이트된 HTML span-<span class = "defaultcurrency"> $ 7 </ span>을 살펴보면 이 웹 페이지가 default-currency 클래스를 사용하여 요금 가격을 나열한다는 것을 알 수 있다. 이제 이 속성을 사용하여 다양한 GitHub 계획의 가격을 추출한다.

2017.03.13

Page 19.

앞에서 설명한 것처럼 정보를 추출하는 적절한 방법을 찾아야 한다. 따라서 이 예제에서는 우선 https://github.com/pricing/ 페이지에 대한 HTML 트리를 가져 왔다. 페이지의 내용(문자열 형식)을 HTML 형식으로 변환하는 fromstring() 메서드를 사용하여 트리를 가져 왔다.

그런 다음 lxml 모듈과 tree_xpath() 메소드를 사용하고 가격 및 가격 플랜을 얻기 위해 default-currency 클래스와 pricing-card-name display-heading-3을 찾는다.

실제 가격 데이터를 선택하기 위해 가격 플랜과 //span[@class="default-currency"] XPath를 탐색하는 완전한 XPath h3[@class='class-name']를 사용하는 방법을 확인한다. 요소(elements)가 선택되면 파이썬 목록으로 반환된 텍스트 데이터를 인쇄했다.

그게 전부다. 필요한 데이터를 얻기 위해 GitHub 페이지를 스크랩했다. 좋고 간단하다.

Page 20.

There's more...

웹 스크래퍼가 무엇인지, 그리고 웹에서 흥미로운 정보를 추출하는 방법에 대해 다뤘다. 또한 그들이 웹 크롤러와 다른 점을 이해했다. 하지만 항상 더 많은 것이 있다.

웹 스크래핑은 웹 페이지에서 HTML 컨텐츠를 파싱하여 데이터를 흥미롭게 할 때까지는 추출할 수 없다.(수정) 다음 레시피에서는 HTML 및 XML 컨텐츠를 자세히 파싱하는 방법을 살펴보자.

웹 컨텐츠 파싱 및 추출

이제 여러 URL에 대한 HTTP 요청을 생성하는 것에 대해 확신한다. 웹 스크래핑의 간단한 예제를 살펴본다.

그러나 WWW는 여러 데이터 형식의 페이지로 구성된다. 웹과 스크랩하고 데이터를 이해하려면, 웹에서 데이터를 사용할 수 있는 다양한 형식을 파싱하는 방법을 알아야한다.

이 레시피에서는 어떻게 해야하는지 살펴볼 것이다.

시작하기

웹의 데이터는 대부분 HTML 혹은 XML 형식이다. 웹 컨텐츠를 파싱하는 방법을 이해하기 위해 HTML 파일을 예제로 살펴볼 것이다. 특정 HTML 요소를 선택하고, 원하는 데이터를 추출하는 방법을 다룬다. 이 레시피에서는 파이썬의 BeautifulSoup 모듈을 설치해야한다. BeautifulSoup 모듈은 HTML 컨텐트를 파싱하는 훌륭한 작업을 수행할 수 있는 가장 포괄적인 파이썬 모듈 중 하나이다. 이제 시작하자.

How to do it...

1. 파이썬 인스턴스에 BeautifulSoup을 설치하는 것으로 시작한다. 다음 커맨드는 모듈을 설치하는 데 도움이 될 것이다. 최신 버전인 beautifulsoup4를 설치한다.

pip install beautifulsoup4

2. 다음 HTML 파일을 살펴보면 HTML 컨텐츠를 파싱하는 방법을 배우는 데 도움이 된다.

<소스>

3. 이 파일의 이름을 python.html으로 정하자. HTML 파일은 수작업으로 생성되어 파싱의 여러 방법을 통해 필요한 데이터를 얻을 수 있다. Python.html은 다음과 같은 전형적인 HTML 태그를 가진다.

-<head> - <title>과 같은 모든 헤드 요소의 컨테이너이다.
-<body> - HTML 문서의 본문을 정의한다.
-<p> -이 요소는 HTML로 단락을 정의한다.
-<span> - 문서의 인라인 요소를 그룹화하는 데 사용된다.
-<strong> -이 태그 아래에 있는 텍스트에 볼드(bold) 스타일을 적용하는 데 사용된다.
-<a> - 하이퍼링크(hyperlink) 혹은 앵커(anchor)를 나타내며 하이퍼링크를 가리키는 <href>를 포함한다.
-<class> - 스타일 시트의 클래스를 가리키는 속성(attribute)이다.
-<div id> - 다른 페이지 요소를 캡슐화하고 내용을 섹션(sections)으로 구분하는 컨테이너이다. 모든 섹션은 속성 id로 식별할 수 있다.

Page 22.

4. 브라우저에서 이 HTML을 열면 어떻게 보일 것입니다.(확인)

<사진>

5. 이제 이 HTML 파일을 파싱할 파이썬 코드를 작성해보자. 먼저 BeautifulSoup 객체를 생성한다.

<<TIP 시작>>
항상 파서를 정의해야한다. 이 경우에는  파서로 lxml을 사용했다. 파서는 지정된 형식으로 파일을 읽는 데 도움이되므로 쿼리 데이터를 쉽게 사용할 수 있다.
<<TIP 끝>>

<소스>

앞의 코드의 출력은 다음 스크린 샷에서 확인할 수 있다. 

<그림>

6. 좋아. 그러나 데이터는 어떻게 검색하지? 데이터를 검색하기 전에, 필요한 데이터가 포함된 HTML 요소를 선택해야한다.

2017.03.16

7. 다양한 방식으로 HTML 요소를 선택하거나 찾을 수 있다. ID, CSS 혹은 태그가 있는 요소를 선택할 수 있다. 다음 코드는 python.html을 사용하여 이 개념을 보여준다.

<소스>

Page 23.

앞의 코드의 출력은 다음 스크린샷에서 확인할 수 있다.

<그림>

8. 이제 HTML 파일에서 실제 내용을 살펴 보자. 다음은 관심있는 데이터를 추출할 수 있는 몇 가지 방법이다.

<소스>

앞의 코드 일부의 출력은 다음과 같다. 

<그림>

우아! HTML 요소에서 원하는 모든 텍스트를 어떻게 얻었는지 살펴본다.

How it works...

이 레시피에서는 ID, CSS 혹은 태그를 기반으로 다른 HTML 요소를 찾고 선택하는 기술을 다뤘다. 

이 레시피의 두 번째 코드 예제에서는 find_all('a')을 사용하여 HTML 파일에서 모든 앵커 요소를 가져온다. find_all() 메소드를 사용했을 때, 배열의 일치하는 다양한 인스턴스가 있다. select() 메서드를 사용하면 요소에 직접 도달 할 수 있다. 

Page 24.

또한 div Id를 사용하여 HTML 요소를 선택하려면 find('div', <divId>) 혹은 select(<divId>)를 사용했다. find() 및 select() 메소드를 사용하여 두 가지 방법으로 div ID #inventor로 inventor 요소를 선택하는 방법에 유의하자. 실제로 select 메소드는 select(<class-name>)로 사용하여 CSS 클래스 이름을 가진 HTML 요소를 선택할 수도 있다. 예제에서 wow 요소를 선택하기 위해 이 메소드를 사용했다.

세 번째 예제 코드에서는 HTML 페이지의 모든 앵커 요소를 검색하고 soup.find_all('a')[0]을 사용하여 첫 번째 인덱스를 살펴봤다. 앵커 태그가 하나뿐이므로 인덱스 0을 사용하여 해당 요소를 선택했지만 다양한 앵커 태그가 존재하는 경우 인덱스 1을 사용하여 접속할 수도 있다. getText()와 같은 메소드와 text(위의 예제에서 볼 수 있듯이)와 같은 속성은 요소에서 실제 내용을 추출하는 데 유용하다.

There's more...

멋지다. 그래서 파이썬으로 웹 페이지(혹은 HTML 페이지)를 파싱하는 방법을 살펴봤다. ID, CSS 혹은 태그를 사용하여 HTML 요소를 선택하거나 찾는 방법도 다뤘다. HTML에서 필수 컨텐츠를 추출하는 방법의 예제도 살펴봤다. 웹에서 페이지 혹은 파일의 내용을 다운로드하려면 어떻게 해야하나? 다음 레시피에서 그것을 달성할 수 있는지 살펴보자.

웹에서 컨텐츠 다운로드

이전 레시피에서는 HTTP 요청을 만드는 방법을 보았고 웹 응답(web response)을 파싱하는 방법도 살펴봤다. 앞으로 나아가 웹에서 컨텐츠를 다운로드할 차례이다. WWW는 단지 HTML 페이지에 관한 것이 아니라는 것을 알고 있다. 많은 다른 형식에는 텍스트 파일, 문서 및 이미지와 같은 다른 리소스가 포함된다. 여기 이 레시피에서는 예제를 사용하여 파이썬에서 이미지를 다운로드하는 방법을 살펴볼 것이다.

시작하기

이미지를 다운로드하려면 BeautifulSoup과 urllib2라는 두 개의 Python 모듈이 필요하다. urrlib2 대신 요청 모듈을 사용할 수 있지만 urllib2에 대한 정보를 HTTP 요청에 사용할 수 있는 대안으로 배울 수 있으므로 자랑할 수 있습니다.(확인)

Page 25.

2017.03.20

시작하기

1. 이 레시피를 시작하기 전에 두 가지 질문에 답해야한다. 어떤 종류의 이미지를 다운로드하고 싶은가? 웹의 어떤 위치에서 이미지를 다운로드할까? 이 레시피에서는 구글(https://google.com) 이미지 검색에서 아바타 동영상 이미지를 다운로드한다. 검색 기준과 일치하는 상위 5 개의 이미지를 다운로드한다. 이를 위해 파이썬 모듈을 임포트해서 필요한 변수를 정의한다.

<소스>

2. 

