1. 웹 작업

인터넷없는 삶을 상상해본 적이 있을까? 음식 주문에 대한 정보를 교환하는 것부터 거의 모든 것이 오늘날 인터넷에 크게 의존한다. 흥미로운 World Wide Web의 세계를 살펴보고 파이썬(Python) 모듈을 사용하여 상호 작용할 수 있는 다양한 방법을 살펴보자.

1장에서는 다음 내용을 살펴본다.

-HTTP 요청생성
-웹 스크래핑(web scraping)에 대한 간략한 설명
-웹 컨텐츠(web content) 파싱(Parsing) 및 추출(extracting)
-웹 컨텐츠 다운로드
-서드 파티(third-party) REST API 작업
-파이썬의 비동기 HTTP 서버
-셀레늄 바인딩(selenium bindings)을 사용한 웹 자동화
-웹 스크래핑과 리드 생성 자동화

소개

인터넷은 삶을 매우 쉽게 만들어주기 때문에 때로는 그 힘을 깨닫지 못한다. 친구 상태를 확인, 부모님 전화, 중요한 비즈니스 이메일(e-mail)에 응답하거나 게임을 하는 등- 오늘날 거의 모든 일에 대해 WWW(World Wide Web)를 사용한다.

Page 13.

이 레시피를 실행하려면 파이썬 v2.7을 설치해야한다. 일단 설치되면 파이썬 pip를 설치해야한다. PIP는 Pip Installs Packages의 약자로 컴퓨터에 필요한 파이썬 패키지를 다운로드하고 설치하는 데 사용할 수 있는 프로그램이다. 마지막으로 HTTP 요청을 하기 위해 requests 모듈이 필요하다.

requests 모듈을 설치하여 시작할 것이다(운영 체제를 기반으로 머신에서 수행할 수 있도록 파이썬 및 pip 설치를 남겨둘 것이다). 다른 전제 조건은 없다. 이제, 서둘러서 시작하자.

사용 방법

1. Linux/Mac 컴퓨터에서 터미널(Terminal)로 이동하여 다음 커맨드를 실행한다.

pip install -U requests

파이썬 사이트 패키지에 대한 권한이 없는 경우 sudo만 사용해야한다. 그렇지 않으면 sudo가 필요하지않다.(확인)

2. 다음 코드는 파이썬의 requests 모듈에서 HTTP GET 요청을하는 데 도움이 된다.

import requests r =
requests.get('http://ip.jsontest.com/')
print("Response object:", r)
print("Response Text:", r.text)

3. 다음 출력을 확인한다.

<그림>

Page 14.

4. 데이터 페이로드(data payload)로 HTTP GET 요청을 생성하는 것은 요청에서 간단하다. 다음 코드는 이를 달성하는 데 도움이 된다. 전송될 URL 요청을 확인할 수도 있다.

payload = {'q': 'chetan'} r =
requests.get('https://github.com/search', params=payload)
print("Request URL:", r.url)

<그림>

5. 이제 requests 모듈을 사용하여 HTTP POST 호출을 생성해보자. 이는 웹 사이트에 로그인 혹은 가입 양식을 채우고 포스트(POST) 하는 것과 유사하다.

payload = {'key1': 'value1'} r =
requests.post("http://httpbin.org/post", data=payload)
print("Response text:", r.json())

<그림>

Page 15.

오류(errors) 및 예외 처리(exceptions)는 요청에 대해서도 매우 편리하다. 다음 단편적인 코드는 오류 처리의 예제를 보여준다. 머신에서 인터넷 연결없이 코드를 실행하면 예외가 발생한다. 예외 처리기(exception handler)는 예외를 캐치(catch)하고 예상대로 새 연결을 설정하지 못했다.(확인)

try:
    r = requests.get("http://www.google.com/")
except requests.exceptions.RequestException as e:
    print("Error Response:", e.message)

어떻게 작동 하는가?

이 레시피에서는 파이썬의 requests 모듈을 사용하여 다양한 유형의 HTTP 요청을 생성하는 방법을 살펴본다. 이 코드가 어떻게 작동하는지 살펴보자.

- 첫 번째 예제에서는 http://ip.jsontest.com에 GET 요청을 전송하고 응답 코드와 응답 텍스트를 받았다. 인터넷에서 컴퓨터의 현재 IP 주소를 반환한다.
- 두 번째 예제에서는 페이로드 데이터로 HTTP GET 요청을 수행했다.    

2017.03.09

웹 스크래핑(web scraping) 살펴보기

웹 스크래핑을 수행하는 방법을 배우기 전에 스크래핑의 의미를 이해하자. 웹 세계에서 스크래핑은 컴퓨터 프로그램의 도움으로 상기 포맷으로 필요한 정보를 추출하려는 의도로 웹 사이트의 페이지를 가려내는 방법이다. 예를 들어 블로그에 게시된 모든 기사의 제목과 날짜를 가져 오려면 블로그를 통해 긁어 모으고 필요한 데이터를 가져오고, 요청에 기반이 되는 데이터베이스(database) 혹은 플랫 파일(flat file)에 저장하는 프로그램을 생성할 수 있다.

웹 스크래핑(Web scraping)은 종종 웹 크롤링(web crawling)과 혼동된다. 웹 크롤러(web crawler)는 웹 인덱싱의 목적으로 웹을 체계적으로 탐색하고 사용자가 웹을보다 효과적으로 검색할 수 있도록 웹 페이지 색인을 위한 검색 엔진에서 사용되는 봇이다.

그러나 스크래핑은 쉽지 않다. 우리에게 흥미로운 데이터는 XML 태그 혹은 HTML 태그에 포함된 특정 형식의 블로그 혹은 웹 사이트에서 사용할 수 있다. 따라서 필요한 데이터를 추출하기 전에 형식을 알아야 한다. 또한 웹 스크래퍼는 추출된 데이터를 나중에 처리하기 위해 저장해야하는 형식을 알아야한다. 브라우저 표시가 같을지라도 HTML 혹은 XML 형식이 변경되면 스크래핑 코드는 실패한다는 것을 이해하는 것도 중요하다.

2017.03.10

웹 스크래핑의 적법성

웹 스크래핑은 항상 법적인 측면에서 스캐너 아래에 있었다. 웹 스크래핑을 할 수 있을까? 법적 혹은 윤리적인 방법은 무엇인가? 수익 창출을 위해 스크래핑으로 확보한 데이터를 사용할 수 있을까?

이 주제는 많은 논의를 거쳤지만, 웹에서 저작권 정보를 스크랩하거나, 컴퓨터 사기 및 남용 방지법을 위반하거나, 웹 사이트의 서비스 약관을 위반하면 웹 스크래핑 관련 문제가 발생할 수 있다. 예를 들어, 공개 데이터를 가져 오기 위해 웹을 스크래핑하고 있다면 여전히 괜찮을 것이다. 그러나 그것은 매우 문맥적이며 사용자가 스크래핑하는 것과 데이터를 사용하는 방법에 대해 주의해야 한다.

Page 17.

다음은 데이터 스크래핑에 대한 웹의 몇 가지 지침이다.

-https://en.wikipedia.org/wiki/Web_scraping#Legal_issues
-https://www.quora.com/What-is-the-legality-of-web-scraping

Getting ready

https://github.com/ 웹 사이트에서 가격 데이터를 사용하여 파이썬으로 웹 스크래핑을 살펴본다. 이것은 매우 사소한 예제이지만 스크래핑 하는 데 걸리는 시간을 단축시킨다. 이 파이썬 레시피를 사용하여 흥미로운 데이터를 시작하고 스크랩하자.

How to do it

1. 컴퓨터에서 구글 크롬 브라우저를 실행시키고 https://github.com/pricing/ 웹 페이지를 연다. 이 페이지에서는 개인(Personal), 조직(Organization) 및 엔터프라이즈(Enterprise)와 같은 여러 가지 가격 정책을 확인할 수 있다.
2. 이제 브라우저에서 Personal 계획의 가격을 마우스 오른쪽 버튼으로 클릭하고 다음 스크린 샷처럼 Inspect 요소를 클릭한다.

<그림>

Page 18.

3. Inspect를 클릭하면 크롬 브라우저 콘솔 로그가 열리며 다음과 같은 GitHub 가격 페이지의 HTML 구조를 이해하는 데 도움이 된다. 

<그림>

4. 하이라이트된 HTML span-<span class = "defaultcurrency"> $ 7 </ span>을 살펴보면 이 웹 페이지가 default-currency 클래스를 사용하여 요금 가격을 나열한다는 것을 알 수 있다. 이제 이 속성을 사용하여 다양한 GitHub 계획의 가격을 추출한다.

2017.03.13

Page 19.

앞에서 설명한 것처럼 정보를 추출하는 적절한 방법을 찾아야 한다. 따라서 이 예제에서는 우선 https://github.com/pricing/ 페이지에 대한 HTML 트리를 가져 왔다. 페이지의 내용(문자열 형식)을 HTML 형식으로 변환하는 fromstring() 메서드를 사용하여 트리를 가져 왔다.

그런 다음 lxml 모듈과 tree_xpath() 메소드를 사용하고 가격 및 가격 플랜을 얻기 위해 default-currency 클래스와 pricing-card-name display-heading-3을 찾는다.

실제 가격 데이터를 선택하기 위해 가격 플랜과 //span[@class="default-currency"] XPath를 탐색하는 완전한 XPath h3[@class='class-name']를 사용하는 방법을 확인한다. 요소(elements)가 선택되면 파이썬 목록으로 반환된 텍스트 데이터를 인쇄했다.

그게 전부다. 필요한 데이터를 얻기 위해 GitHub 페이지를 스크랩했다. 좋고 간단하다.

Page 20.

There's more...

웹 스크래퍼가 무엇인지, 그리고 웹에서 흥미로운 정보를 추출하는 방법에 대해 다뤘다. 또한 그들이 웹 크롤러와 다른 점을 이해했다. 하지만 항상 더 많은 것이 있다.

웹 스크래핑은 웹 페이지에서 HTML 컨텐츠를 파싱하여 데이터를 흥미롭게 할 때까지는 추출할 수 없다.(수정) 다음 레시피에서는 HTML 및 XML 컨텐츠를 자세히 파싱하는 방법을 살펴보자.

웹 컨텐츠 파싱 및 추출

이제 여러 URL에 대한 HTTP 요청을 생성하는 것에 대해 확신한다. 웹 스크래핑의 간단한 예제를 살펴본다.

그러나 WWW는 여러 데이터 형식의 페이지로 구성된다. 웹과 스크랩하고 데이터를 이해하려면, 웹에서 데이터를 사용할 수 있는 다양한 형식을 파싱하는 방법을 알아야한다.

이 레시피에서는 어떻게 해야하는지 살펴볼 것이다.

시작하기

웹의 데이터는 대부분 HTML 혹은 XML 형식이다. 웹 컨텐츠를 파싱하는 방법을 이해하기 위해 HTML 파일을 예제로 살펴볼 것이다. 특정 HTML 요소를 선택하고, 원하는 데이터를 추출하는 방법을 다룬다. 이 레시피에서는 파이썬의 BeautifulSoup 모듈을 설치해야한다. BeautifulSoup 모듈은 HTML 컨텐트를 파싱하는 훌륭한 작업을 수행할 수 있는 가장 포괄적인 파이썬 모듈 중 하나이다. 이제 시작하자.

How to do it...

1. 파이썬 인스턴스에 BeautifulSoup을 설치하는 것으로 시작한다. 다음 커맨드는 모듈을 설치하는 데 도움이 될 것이다. 최신 버전인 beautifulsoup4를 설치한다.

pip install beautifulsoup4

2. 다음 HTML 파일을 살펴보면 HTML 컨텐츠를 파싱하는 방법을 배우는 데 도움이 된다.

<소스>

3. 이 파일의 이름을 python.html으로 정하자. HTML 파일은 수작업으로 생성되어 파싱의 여러 방법을 통해 필요한 데이터를 얻을 수 있다. Python.html은 다음과 같은 전형적인 HTML 태그를 가진다.

-<head> - <title>과 같은 모든 헤드 요소의 컨테이너이다.
-<body> - HTML 문서의 본문을 정의한다.
-<p> -이 요소는 HTML로 단락을 정의한다.
-<span> - 문서의 인라인 요소를 그룹화하는 데 사용된다.
-<strong> -이 태그 아래에 있는 텍스트에 볼드(bold) 스타일을 적용하는 데 사용된다.
-<a> - 하이퍼링크(hyperlink) 혹은 앵커(anchor)를 나타내며 하이퍼링크를 가리키는 <href>를 포함한다.
-<class> - 스타일 시트의 클래스를 가리키는 속성(attribute)이다.
-<div id> - 다른 페이지 요소를 캡슐화하고 내용을 섹션(sections)으로 구분하는 컨테이너이다. 모든 섹션은 속성 id로 식별할 수 있다.

Page 22.

4. 브라우저에서 이 HTML을 열면 어떻게 보일 것입니다.(확인)

<사진>

5. 이제 이 HTML 파일을 파싱할 파이썬 코드를 작성해보자. 먼저 BeautifulSoup 객체를 생성한다.

<<TIP 시작>>
항상 파서를 정의해야한다. 이 경우에는  파서로 lxml을 사용했다. 파서는 지정된 형식으로 파일을 읽는 데 도움이되므로 쿼리 데이터를 쉽게 사용할 수 있다.
<<TIP 끝>>

<소스>

앞의 코드의 출력은 다음 스크린 샷에서 확인할 수 있다. 

<그림>

6. 좋아. 그러나 데이터는 어떻게 검색하지? 데이터를 검색하기 전에, 필요한 데이터가 포함된 HTML 요소를 선택해야한다.

2017.03.16

7. 다양한 방식으로 HTML 요소를 선택하거나 찾을 수 있다. ID, CSS 혹은 태그가 있는 요소를 선택할 수 있다. 다음 코드는 python.html을 사용하여 이 개념을 보여준다.

<소스>

Page 23.

앞의 코드의 출력은 다음 스크린샷에서 확인할 수 있다.

<그림>

8. 이제 HTML 파일에서 실제 내용을 살펴 보자. 다음은 관심있는 데이터를 추출할 수 있는 몇 가지 방법이다.

<소스>

앞의 코드 일부의 출력은 다음과 같다. 

<그림>

우아! HTML 요소에서 원하는 모든 텍스트를 어떻게 얻었는지 살펴본다.

How it works...

이 레시피에서는 ID, CSS 혹은 태그를 기반으로 다른 HTML 요소를 찾고 선택하는 기술을 다뤘다. 

이 레시피의 두 번째 코드 예제에서는 find_all('a')을 사용하여 HTML 파일에서 모든 앵커 요소를 가져온다. find_all() 메소드를 사용했을 때, 배열의 일치하는 다양한 인스턴스가 있다. select() 메서드를 사용하면 요소에 직접 도달 할 수 있다. 

Page 24.

또한 div Id를 사용하여 HTML 요소를 선택하려면 find('div', <divId>) 혹은 select(<divId>)를 사용했다. find() 및 select() 메소드를 사용하여 두 가지 방법으로 div ID #inventor로 inventor 요소를 선택하는 방법에 유의하자. 실제로 select 메소드는 select(<class-name>)로 사용하여 CSS 클래스 이름을 가진 HTML 요소를 선택할 수도 있다. 예제에서 wow 요소를 선택하기 위해 이 메소드를 사용했다.

세 번째 예제 코드에서는 HTML 페이지의 모든 앵커 요소를 검색하고 soup.find_all('a')[0]을 사용하여 첫 번째 인덱스를 살펴봤다. 앵커 태그가 하나뿐이므로 인덱스 0을 사용하여 해당 요소를 선택했지만 다양한 앵커 태그가 존재하는 경우 인덱스 1을 사용하여 접속할 수도 있다. getText()와 같은 메소드와 text(위의 예제에서 볼 수 있듯이)와 같은 속성은 요소에서 실제 내용을 추출하는 데 유용하다.

There's more...

멋지다. 그래서 파이썬으로 웹 페이지(혹은 HTML 페이지)를 파싱하는 방법을 살펴봤다. ID, CSS 혹은 태그를 사용하여 HTML 요소를 선택하거나 찾는 방법도 다뤘다. HTML에서 필수 컨텐츠를 추출하는 방법의 예제도 살펴봤다. 웹에서 페이지 혹은 파일의 내용을 다운로드하려면 어떻게 해야하나? 다음 레시피에서 그것을 달성할 수 있는지 살펴보자.

웹에서 컨텐츠 다운로드

이전 레시피에서는 HTTP 요청을 만드는 방법을 보았고 웹 응답(web response)을 파싱하는 방법도 살펴봤다. 앞으로 나아가 웹에서 컨텐츠를 다운로드할 차례이다. WWW는 단지 HTML 페이지에 관한 것이 아니라는 것을 알고 있다. 많은 다른 형식에는 텍스트 파일, 문서 및 이미지와 같은 다른 리소스가 포함된다. 여기 이 레시피에서는 예제를 사용하여 파이썬에서 이미지를 다운로드하는 방법을 살펴볼 것이다.

시작하기

이미지를 다운로드하려면 BeautifulSoup과 urllib2라는 두 개의 Python 모듈이 필요하다. urrlib2 대신 요청 모듈을 사용할 수 있지만 urllib2에 대한 정보를 HTTP 요청에 사용할 수 있는 대안으로 배울 수 있으므로 자랑할 수 있습니다.(확인)

Page 25.

2017.03.20

시작하기

1. 이 레시피를 시작하기 전에 두 가지 질문에 답해야한다. 어떤 종류의 이미지를 다운로드하고 싶은가? 웹의 어떤 위치에서 이미지를 다운로드할까? 이 레시피에서는 구글(https://google.com) 이미지 검색에서 아바타 동영상 이미지를 다운로드한다. 검색 기준과 일치하는 상위 5 개의 이미지를 다운로드한다. 이를 위해 파이썬 모듈을 임포트해서 필요한 변수를 정의한다.

<소스>

2. 이제 URL 매개변수와 적절한 헤더를 사용하여 BeautifulSoup 객체를 생성한다. Python urllib 모듈로 HTTP 호출을 생성하는 동안 User-Agent 사용법을 살펴본다. 요청 모듈은 HTTP 호출을 생성하는 동안 자신의 User-Agent를 사용한다.

<소스>

3. 구글 이미지는 도메인 이름 http://www.gstatic.com/에서 정적 컨텐츠로 호스팅된다. 따라서 BeautifulSoup 객체를 사용하여 소스 URL이 http://www.gstatic.com/을 포함하는 모든 이미지를 찾으려고 한다. 다음 코드는 틀림없이 동일한 코드이다.

<소스>

Page 26.

앞의 코드의 부분 출력은 다음 스크린 샷에서 살펴볼 수 있다. 상위 5개 이미지에 대해 웹에서 이미지 소스 URL을 얻는 방법을 살펴본다.

<그림>

4. 이제 모든 이미지의 소스 URL을 얻었으므로 다운로드 하자. 다음 파이썬 코드는 read()를 위한 urlopen() 메서드를 사용하여 이미지를 하여 로컬 파일 시스템에 다운로드한다. 

<소스>

5. 이미지가 다운로드되면 편집기에서 이미지를 확인할 수 있다. 다음 스냅샷은 다운로드 한 상위 5개의 이미지를 보여 주며 Project_3.jpg는 다음과 같다.

<그림>

Page 27.

동작원리

따라서 이 레시피에서는 웹에서 컨텐츠를 다운로드하는 방법을 살펴본다. 먼저 다운로드할 매개변수를 정의한다. 매개변수는 다운로드할 수 있는 리소스를 사용할 수 있는 위치와 다운로드할 컨텐츠의 종류를 정의하는 구성과 같다. 이 예제에서는 Avatar 동영상 이미지와 구글에서 다운로드해야 하는 이미지를 정의했다.(확인)

2017.03.29

그 후 urllib2 모듈을 사용하여 URL 요청을 하는 BeautifulSoup 객체를 생성했다. 사실, urllib2.Request()는 헤더와 URL 자체와 같은 구성으로 요청을 준비하며 urllib2.urlopen()은 실제로 요청을 한다. urlopen() 메서드의 HTML 응답을 래핑하고 HTML 응답을 파싱할 수 있도록 BeautifulSoup 객체를 생성했다.

다음으로, soup 객체를 사용하여 HTML 응답에 있는 상위 5개의 이미지를 검색했다. find_all() 메소드를 사용하여 img 태그를 기반으로 이미지를 검색했다. 알다시피 find_all()은 구글(Google)에서 사진을 사용할 수 있는 이미지 URL 목록을 반환한다. 알다시피 find_all()은 구글(Google)에서 사진을 사용할 수 있는 이미지 URL 목록을 반환한다.

마지막으로 모든 URL을 반복하고 URL에서 urlopen() 메서드를 사용하여 이미지를 읽는다.(확인) Read()는 원시 형식의 이미지를 이진 데이터로 반환한다. 그런 다음 이 원시 이미지를 사용하여 로컬 파일 시스템의 파일에 기록했다. 또한 로컬 파일시스템에서 고유하게 식별되도록 이미지(실제로 자동 증가)의 이름을 지정하는 논리를 추가했다.

멋지다! 정확히 우리가 달성하기를 원했던 것! 이제 분담금을 조금 내고 다음 레시피에서 탐색할 수 있는 것이 무엇인지 살펴보자.(확인)

써드파티(third-party) REST API 작업

스크래핑, 크롤링 및 파싱에 대한 내용을 다뤘으므로 써드파티 API를 사용하는 파이썬으로 할 수 있는 또 다른 흥미로운 작업을 할 시간이다. 많은 사람들이 인식하고 REST API에 대한 기본적인 지식을 가지고 있다고 가정한다. 이제 시작하자.

Page 28.

2017.03.30

시작하기

이해를 돕기 위해 GitHub 기스트(gists)를 사용한다. GitHub의 기스트(Gists)는 작업을 공유하는 가장 좋은 방법이며 동료 혹은 작은 애플리케이션(app)에 개념을 이해할 수 있는 여러 파일을 제공하는 작은 코드 조각이다. GitHub는 기스트의 생성(creation), 목록(listing), 삭제(deleting) 및 수정(updating)을 허용하며 GitHub REST API로 작업하는 고전적인 사례를 제시한다.

따라서 이 절에서는 자신의 requests 모듈을 사용하여 GitHub REST API에 대한 HTTP 요청을 작성하여 기스트를 생성, 수정, 목록 혹은 삭제한다.

다음 단계는 파이썬을 사용한 GitHub REST API를 사용하는 방법을 살펴볼 것이다.

작동원리(동작원리?)

1. GitHub REST API를 사용하려면 Personal access token을 생성해야한다. 그런 다음 https://github.com/에 로그인하고 https://github.com/settings/tokens로 이동한 다음 Generate new token을 클릭한다.

<그림>

Page 29.

2. 이제 New personal  access token 페이지가 나타난다. 페이지 상단에 설명을 입력하고 주어진 범위 중 gists 옵션을 확인한다. 범위(scope)는 토큰에 대한 접속을 나타낸다. 예를 들어 gists를 선택했다면 GitHub API를 사용하여 gists 자원을 동작할 수 있지만 repo 혹은 사용자와 같은 다른 자원은 사용할 수 없다. 이 레시피의 경우 gists 범위는 필요한 것이다.

<그림>

3. Generate token을 클릭하면 개인 액세스 토큰(personal access token)이 포함된 화면이 나타난다. 이 토큰을 기밀로 유지한다.
4. 사용 가능한 토큰을 접속하여 API 작업을 시작하고 새로운 기스트를 생성한다. create를 사용하여 새 리소스를 추가하고 이렇게하려면 다음 코드와 같이 GitHub API에 대한 HTTP POST 요청을 생성한다.

<소스>

Page 30.

<소스>

5. GitHub의 기스트 페이지로 가면 새로 생성된 기스트를 확인할 수 있다. And voila, it's available.(확인)

<그림>

6. 안녕, GitHub API를 사용하여 기스트를 성공적으로 생성했다. 멋지다, 하지만 이제 이 기스트(gist)를 확인할 수 있을까? 앞의 예제에서는, 새로 생생된 기스트의 URL도 출력했다. 형식은 https://gist.github.com/<username>/<gist_id>이다. 이제 이 gist_id를 사용하여 gist의 세부 정보 즉, gist_id에 대해 HTTP GET 요청을 생성하는 것을 의미한다.

<소스>

Page 31.

<소스>

7. HTTP POST 요청으로 새로운 기스트를 생성했고 이전 단계에서 HTTP GET 요청을 통해 기스트의 세부 사항을 얻었다. 이제 HTTP PATCH 요청으로 이 기스트를 업데이트 하자. 

<정보표시 시작>
많은 써드파티 라이브러리가 PUT 요청을 사용하여 리소스를 업데이트하도록 선택하지만 GitHub에서 선택한대로 이 작업에 HTTP PATCH를 사용할 수도 있다.
<정보표시 끝>

8. 다음 코드는 기스트를 업데이트하는 방법을 보여준다.

<소스>

Page 32.

2017.03.31

9. 이제, GitHub 로그인을 보고 이 기스트를 탐색하면 그 기스트의 내용이 업데이트된다. 굉장하다! 스크린 샷에서 Revisions을 확인하는 것을 잊지말자-revision 2로 업데이트 됐다.

<그림>

10. 이제 가장 파괴적인 API 작업이 있습니다. yes는 기스트를 삭제한다. GitHub는 /gists/<gist_id> 리소스에서 HTTP DELETE 작업을 사용하여 기스트를 제거하기 위한 API를 제공한다. 다음 코드는 기스트를 삭제하는 데 도움이 된다.

<소스>

Page 33.

11. GitHub 웹사이트에서 기스트가 사용가능한지 빨리 살펴보자. 웹 브라우저에서 기스트 URL을 탐색하여 이를 수행할 수 있다. 브라우저에서 말하는 것은 무엇인가? 404 리소스를 찾을 수 없다는 메시지가 표시되므로 성공적으로 기스트를 삭제했다! 다음 스크린 샷을 참조한다.

<그림>

12. 마지막으로, 당신의 계정에 있는 모든 기스트 목록을 확인하자. 이를 위해 /users/<username>/gists 리소스에서 HTTP GET API 호출한다.

<소스>

Page 34.

내 계정에 대한 위 코드의 출력은 다음과 같다.

<그림>

작동방법

파이썬 requests 모듈은 GitHub의 리소스에서 HTTP GET/POST/PUT/PATCH 및 DELETE API 호출을 하는 데 도움이 된다. REST 용어(terminology)에서 HTTP 동사라고도 하는 이 작업은 URL 리소스에 대한 특정 작업을 수행한다.

Page 35.

예제에서처럼 HTTP GET 요청은 기스트를 나열하는 데 도움이 되고 POST는 새로운 기스트를 생성하고 PATCH는 기스트를 업데이트하며 DELETE는 기스트를 완전히 제거한다. 따라서 이 방법에서는 오늘날 WWW의 필수요소인 파이썬을 사용하는 써드파티 REST API로 작업하는 방법을 살펴봤다.

see also(~도 보세요)

REST API로 작성된 많은 서드파티 애플리케이션이 있다. GitHub와 같은 방식으로 해보기를 원할 수 있다. 예를 들어, 트위터(Twitter)와 페이스북(Facebook) 모두 훌륭한 API를 가지고 있고 문서는 이해하기 쉽고 사용하기 쉽다. 물론 그들은 파이썬 바인딩을 가진다.

파이썬의 비동기 HTTP 서버

우리가 상호작용하는 많은 웹 애플리케이션은 기본적으로 동기식이다. 클라이언트 연결은 클라이언트가 작성한 모든 요청에 대해 설정되고 호출 가능한 메소드(callable method)는 서버 측에서 호출된다. 서버는 비즈니스 작업을 수행하고 응답 본문(response body)을 클라이언트 소켓에 사용한다. 응답이 고갈되면 클라이언트 연결은 닫힌다. 이러한 모든 작업은 순차적으로 차례로 발생하므로 동기화가 발생한다.(확인)

그러나 오늘날 웹은 동기화 모드 작업에 의존할 수 없다. 웹에서 데이터를 쿼리하고 사용자에 대한 정보를 검색하는 웹 사이트의 경우를 살펴본다(예를 들어 여러분의 웹사이트는 Facebook과의 통합을 허용하며 사용자가 여러분의 웹사이트의 특정 페이지를 방문할 때마다 Facebook 계정에서 데이터를 가져온다.). 이제 동기식 방식으로 이 웹 애플리케이션을 개발하면 클라이언트가 요청할 때마다 서버가 데이터베이스 혹은 네트워크를 통해 I/O 호출을 생성하여 정보를 검색한 다음 다시 클라이언트에 제공한다. 이러한 I/O 요청에 응답하는 데 시간이 오래 걸리면 서버는 응답을 기다리는 동안 블록된다. 일반적으로 웹 서버는 클라이언트의 여러 요청을 처리하는 스레드 풀(thread pool)을 유지한다. 서버가 요청을 처리할 수 있을만큼 오래 대기하면 스레드 풀이 곧 소진되어 서버는 정지된다.

해결책은 무엇인가? 비동기로 전달한다.

Page 36.

시작하기

이 레시피에서는 파이썬으로 개발된 비동기 프레임워크인 Tornado를 사용할 것이다. 파이썬 2와 파이썬 3을 모두 지원하며 원래 FriendFeed(http://blog.friendfeed.com/)에서 개발됐다. Tornado는 논블로킹 네트워크(non-blocking network) I/O를 사용하고 수만개의 실시간 연결(C10K problem)로 확장하는 문제를 해결한다. 나는 이 프레임워크를 좋아하고 코드를 개발하는 것을 즐긴다. 너도 그랬으면 좋겠어! How to do it 절에 들어가기 전에 먼저 다음 커맨드를 실행하여 tornado를 설치해보자.

<소스>

2017.04.03

실행방법

1. 이제 비동기 철학에서 작동하는 자체 HTTP 서버를 개발할 준비가 됐다. 다음 코드는 tornado  웹 프레임워크에서 개발된 비동기 서버를 나타낸다.

<소스>

Page 37.

2. 다음과 같이 서버를 실행한다.

<소스>

3. 이제 서버가 포트 8888에서 실행 중이며 요청을 받을 준비가 됐다.

4. 이제 원하는 브라우저를 실행하고 http://localhost:8888/로 이동한다. 서버에서 다음 출력을 확인할 수 있다.

<그림>

작동원리

비동기식 웹 서버가 이제 가동되어 포트 8888에서 요청을 수락한다. 그러나 이것에 대해 비동기적인 것은 무엇인가? 사실, tornado는 단일 스레드 이벤트 루프의 철학에서 작동한다. 이 이벤트 루프는 이벤트를 폴링하고 해당 이벤트 핸들러로 전달한다.(확인)

앞의 예제에서 앱(app)을 실행하면 ioloop을 실행하여 앱을 시작한다. ioloop은 단일 스레드 이벤트 루프(single-threaded event loop)이며 클라이언트의 요청을 수신한다. @tornado.web.asynchronous로 장식된 get () 메서드를 정의하여 비동기로 생성한다.(확인) 사용자가 http://localhost:8888/에서 HTTP GET 요청을하면 get() 메서드가 트리거되어 내부적으로 http://ip.jsontest.com에 대한 I/O 호출이 수행된다.(확인)

이제 일반적인 동기 웹 서버는 이 I/O 호출의 응답을 기다리고 요청 스레드를 블록한다. 그러나 비동기 프레임워크인 토네이도(tornado)는 작업을 트리거하고 큐에 추가하고 I/O 호출을 생성하고 실행 스레드를 이벤트 루프에 반환한다.

이제 이벤트 루프는 I/O 호출의 응답을 폴링하고 작업 큐를 모니터링한다. 이벤트를 사용할 수 있게되면 이벤트 핸들러 async_callback()을 실행하여 내용과 응답을 인쇄하고 그 후 이벤트 루프를 중지한다.

Page 38.

There's more

토네이도와 같은 이벤트 기반 웹 서버는 커널 수준 라이브러리(kernel-level libraries)를 사용하여 이벤트를 모니터링한다. 이 라이브러리는 kqueue, epoll 등이다. 여러분이 정말로 관심이 있다면, 이것에 대한 더 많은 독서를 해야한다. 다음은 몇 가지 리소스다.

<주소>

셀레늄 바인딩(selenium binding)을 이용한 웹 자동화

지금까지 모든 레시피에서 REST API를 호출하거나 웹에서 컨텐츠를 다운로드하는 등의 HTTP 요청을하기 위한 지정된 URL이 있다. 그러나 정의된 API 리소스가 없거나 작업을 수행하기 위해 웹에 로그인해야하는 서비스가 있다. 이 경우 사용자 세션 혹은 쿠키를 기반으로 여러 다른 컨텐츠를 제공하는 동일한 URL이기 때문에 요청에 대한 제어 권한이 없다. 그럼 우리가 무엇을 해야하나?

그러한 시나리오에서 작업을 수행하기 위해 브라우저 자체를 제어하는 방법은 무엇인가? 브라우저 자체 제어? 흥미롭다. 그렇지?

시작하기

이 레시피에서는 파이썬 셀레늄(selenium) 모듈을 사용한다. Selenium(http://www.seleniumhq.org)은 웹 애플리케이션을 위한 간편한 소프트웨어 프레임워크이며 브라우저 동작을 자동화한다. 셀레늄으로 평범한 작업을 자동화할 수 있다. 셀레늄은 브라우저를 생성하고 인간이 수행하는 것처럼 작업을 수행하는 데 도움이 된다. 셀레늄은 파이어폭스(Firefox), 크롬(Chrome), 사파리(Safari) 및 인터넷 익스플로러(Internet Explorer)와 같이 가장 널리 사용되는 브라우저를 지원한다. 이 레시피에서는 파이썬 셀레늄으로 페이스북(Facebook)에 로그인하는 예제를 살펴보자.

작동원리

1. 파이썬에 바인딩을 위한 셀레늄을 설치하는 것으로 시작한다. 셀레늄 설치는 다음 커맨드를 사용하여 수행할 수 있다.

<소스>

Page 39.

2. 먼저 브라우저 객체를 생성하자. 파어어폭스 인스턴스를 생성하기 위해 파이어폭스 브라우저를 사용한다.

<소스>

3. 다음 스크린샷은 셀레늄 웹 드라이버 객체가 어떻게 생성되었는지 보여준다. 고유한 세션 ID도 있다.

<그림>

4. 다음으로, 페이스북 홈페이지를 찾아 보도록 브라우저에 요청한다. 다음 코드는 이 작업을 수행하는 데 도움이 된다.

<소스>

5. 앞의 코드를 실행하면 파이어폭스 브라우저가 열리고 다음 스크린샷과 같이 페이스북 로그인 페이지에 연결된다.

<그림>

6. 다음 단계에서는 e-mail 및 암호 요소를 탐색하고 적절한 데이터를 입력한다.

<소스>

Page 40.

앞의 코드의 출력은 다음과 같다.

<그림>

7. Email 및 Password 텍스트 입력을 선택하면 올바른 Email 및 Password로 입력한다. 다음 코드는 Email 및 Passowrd 입력을 가능하게 한다.

<소스>
<그림>

8. Email과 Password를 입력 했으므로 마지막으로 폼을 제출하고 Log In 버튼을 클릭하자. ID로 요소(element)를 찾고 요소를 클릭한다.

<소스>

올바른 e-mail ID와 password를 입력했다면 페이스북에 로그인했을 것이다.

작동원리

이 레시피에서는 Selenium WebDriver Python API를 사용했다. WebDriver는 셀레늄 API에 가장 최근에 포함된 것으로, 기본적으로 사용자와 같은 브라우저를 구동한다. 셀레늄 서버를 사용하여 로컬 혹은 원격 머신을 구동할 수 있다. 이 예제에서는 로컬 머신에서 실행했다. 기본적으로 셀레늄 서버는 기본 포트 4444의 로컬 머신에서 실행되며 셀레늄 WebDriver API는 셀레늄 서버와 상호 작용하여 브라우저에서 작업을 수행한다.   

Page 41.

이 레시피에서는 먼저 파이어폭스 브라우저를 사용하여 WebDriver 인스턴스를 생성했다. 그런 다음 WebDriver API를 사용하여 페이스북 홈페이지를 탐색했다. 그런 다음 HTML 페이지를 파싱하고 Email 및 Password 입력 요소를 찾는다. 요소를 어떻게 찾았나? 네, 웹 스크래핑 예제에서 했던 것과 비슷하다. 크롬에 개발자 콘솔이 있으므로 파이어버그(Firebug) 플러그인을 파이어폭스(Firefox)에 설치할 수 있다. 이 플러그인을 사용하여 Email 및 Password의 HTML 요소를 가져올 수 있다. 다음 스크린샷을 확인한다.

<그림>

HTML 요소 이름을 찾으면 WebDriver의 find_element_by_name() 메소드를 사용하여 프로그래밍 방식으로 HTML 요소 객체를 생성했다. WebDriverAPI는 요소 객체에서 작동하고 필요한 텍스트(이 경우 email 및 password)를 입력할 수 있는 send_keys() 메소드가 있다. 마지막 작업은 양식을 제출하는 것이고, Log In 객체를 찾아 클릭하여 그것을 수행했다.

There's more

셀레늄 WebDriver Python 바인딩을 사용하여 매우 기본적인 예제를 살펴봤다. 이제 셀레늄으로 일상적인 작업을 자동화하는 것을 달성할 수 있는 것은 상상에 달려있다.

웹스크래핑으로 리드 생성(lead generation) 자동화

Ryan은 Dely Inc.의 마케팅 매니저이다. Dely는 식품 배달 스타트업으로서 런던 시내에 자리잡으려고 한다. Dely는 물류(logistics)가 우수하며 플랫폼에 레스토랑을 모으고 소비자가 이 레스토랑에서 음식을 주문할 때 Dely는 실제 배송을 책임진다. Dely는 그들이 모든 배달을 하는 것을 기대하고 레스토랑에서 퍼센티지 컷(percentage cut)할 것으로 기대한다.(확인) 보답으로 레스토랑은 물류 측면이 아닌 주방에 대해 생각해야한다. 조심스럽게 생각하면 사실상 모든 레스토랑이 크거나 작을 것으로 예상된다. Dely는 이 레스토랑에 연락하여 플랫폼에 추가하고 배달 요구 사항을 충족시키기를 원한다.

Page 42.

Ryan은 식당과 연락을 취할 책임이 있고 모든 타켓 레스토랑에서 마케팅 캠페인을 실시하기를 원한다. 그러나 그가 이것을 할 수 있기 전에 런던에 있는 모든 레스토랑의 데이터베이스를 생성해야한다. 레스토랑 이름, 주소, 전화 번호와 같은 세부 정보가 있어야 이러한 식당에 연락할 수 있다. Ryan은 그의 모든 리드가 Yelp에 등재되어 있지만 어디서부터 시작해야하는지 알지 못한다. 또한, 그가 수동으로 모든 레스토랑을 살펴보기 시작하면, 엄청난 시간이 걸릴 것이다. 이 장에서 얻은 지식을 바탕으로 Ryan을 리드 생성(lead generation)을 도울 수 있을까?

웹 스크래핑의 적법성

이 장의 초기 부분에서 웹 스크래핑의 법적 측면을 다뤘다. 이것에 대해 다시 한 번 경고하고 싶다. 이 장에서 다루는 예제는 웹 스크래핑을 수행하는 방법을 이해하는 것이다. 또한 여기에서는 공용 데이터에 대해 Yelp를 스크래링하여 이 데이터는 일반적으로 사용할 수 있으므로 이 경우 레스토랑 웹사이트 자체에서 사용할 수 있다. 

준비하기

이제 Ryan의 문제를 살펴보면 런던에있는 모든 레스토랑의 데이터베이스를 자동으로 수집해야 한다. 그래, 맞다. 웹 스크래핑으로 Ryan이 데이터베이스를 구축하는 데 도움이 될 수 있다. 그렇게 쉬울 수 있을까? 이 레시피를 살펴 보자.

이 레시피에서는 추가 모듈이 필요하지 않다. 이 장의 이전 레시피에서 사용한 BeautifulSoup 및 urllib 파이썬 모듈을 사용할 것이다.

Page 43.

작동원리

1. Yelp 웹사이트(https://yelp.com/)로 들어가서 런던 시내 모든 레스토랑을 검색한다. 그렇게하면 런던의 모든 레스토랑 목록을 얻을 수 있다. 검색 기준을 나타내는 URL을 살펴본다. URL은 https://www.yelp.com/search?find_desc=Restaurants&find_loc=London이다. 참조할 수 있도록 다음 스크린샷을 살펴본다.

<그림>

Page 44.

2. 이제 검색 결과에 나타나는 레스토랑 링크를 클릭하면 Ryan이 필요로하는 세부 정보를 얻게된다. Ffiona's Restaurant의 세부 정보는 다음 스크린샷을 살펴본다. 모든 레스토랑에 전용 URL이 있는 방법에 유의한다. 이 경우 https://www.yelp.com/biz/ffionas-restaurant-london?osq=Restaurants이다. 또한 이 페이지에는 레스토랑의 이름, 거리 주소 및 연락처 번호가 있다. Ryan이 캠페인에 필요한 모든 세부 사항이다. 멋지다.

<그림>

3. 좋다. 이제 레스토랑 목록을 얻는 방법을 알게되고 레스토랑 관련 세부 정보도 가져온다. 그러나 자동화된 방식으로 이 목표를 달성할 수 있는 방법이 있을까? 웹 스크래핑 예제에서 보았듯이 이 데이터를 수집할 수 있는 웹 페이지의 HTML 요소를 찾아야 한다.

Page 45.

4. 검색 페이지부터 시작한다. 크롬 브라우저에서 검색 페이지를 연다(https://www.yelp.com/search?find_desc=Restaurants&find_loc=London). 이제 첫 번째 레스토랑의 URL을 마우스 오른쪽 버튼으로 클릭하고 Inspect를 클릭하여 HTML 요소를 가져온다. 다음 스크린샷에서 알 수 있듯이 검색 페이지에 나열된 모든 레스토랑에는 식당의 이름을 나타내는 공통된 CSS 클래스 이름인 biz-name이 있다. 또한 레스토랑의 전용 URL에 대한 href 태그가 포함된다. 스크린샷에서는 Ffiona's Restaurant이라는 이름을 얻었고 href는 레스토랑의 URL인 https://yelp.com/biz/ffionas-restaurant-london?osq=Resturants를 가리킨다.

<그림>

Page 46.

5. 이제 레스토랑의 전용 페이지에서 HTML 요소를 사용하여 식당의 주소 및 연락처를 수집하는 방법을 살펴보자. 동일한 작업을 수행하고, 마우스 오른쪽 버튼으로 클릭하고, Inspect를 클릭하여 거리 주소 및 연락처 번호의 HTML 요소를 가져온다. 참조할 수 있도록 다음 스크린샷을 살펴본다. 거리 주소는 개별 CSS 클래스인 street-address가 있으며 클래스 이름이 biz-phone인 스팬(span) 아래에서 연락처 번호를 사용할 수 있다.

<그림>

Page 47.

6. 굉장해! 이제 자동화된 방식으로 데이터를 스크랩하는 데 사용할 수 있는 모든 HTML 요소가 있다. 이제 구현을 살펴보자. 다음 파이썬 코드는 이러한 작업을 자동화된 방식으로 수행한다.

<소스>

Page 48.

<소스>

7. 좋아! 이제 선행 파이썬 코드를 실행하면 런던(London)의 상위 10개 레스토랑의 세부 정보와 이름, 거리 주소 및 연락처가 제공된다. 다음 스크린샷을 참조한다.

<그림>

Page 49.

앞의 스크린샷에서 Yelp에서 런던에 있는 10개의 레스토랑에 대한 기록을 받았다. Title은 레스토랑의 이름이고 Street Address와 Phone Number는 따로 설명이 필요없다. 굉장해! Ryan을 위해 해냈다.

작동원리

앞의 코드 조각에서 검색 기준을 생성했다. https://yelp.com에서 검색하여 런던의 레스토랑을 찾았다. 이러한 세부 정보로 Yelp의 검색 URL을 얻었다.

그런 다음 urllib 객체를 생성하고 이 검색 URL에서 read()를 위한 urlopen() 메서드를 사용하여 검색 기준과 일치하는 Yelp에서 제공한 모든 레스토랑의 목록을 읽는다. 모든 레스토랑의 목록은 변수 s_html에 저장된 HTML 페이지로 저장된다.

BeautifulSoup 모듈을 사용하여 CSS 요소를 사용한 필요한 데이터의 추출을 시작할 수 있도록 HTML 컨텐츠에 soup 인스턴스를 생성했다.

처음에는 Yelp 검색의 상위 10 개 결과를 탐색하고 레스토랑의 URL을 얻었다. URL 파이썬 목록에 이러한 URL을 저장했다. URL을 얻기 위해 soup_s.select(.biz-name)[:10] 코드를 사용하여 CSS 클래스 이름 biz-name을 선택했다.

또한 레스토랑 URL을 매개변수로 사용하는 scrape() 메소드를 정의했다. 이 메소드에서는 이름, 거리 주소 및 연락처 번호와 같은 레스토랑의 세부 정보를 각각 CSS 클래스 이름인 biz-page-title, street-address 및 biz-phone을 사용하여 읽는다. 정확한 데이터를 얻으려면 title=soup.select(.biz-page-title)를 사용하여 HTML 요소를 선택하고 title[0].getText().strip()로 데이터를 얻는다. select() 메소드는 발견된 요소를 배열로 반환하므로 실제 텍스트를 가져 오기 위해 인덱스 0을 찾아야 한다.

while 루프에서 모든 레스토랑 URL을 반복하고 각 레스토랑의 세부 정보를 얻기 위해 scrape() 메소드를 사용하여 URL을 스크랩했다. 앞의 스크린샷에서 보았듯이 콘솔에 각 레스토랑의 이름, 주소 및 연락처를 출력한다.

스크래핑 프로그램의 성능을 향상시키기 위해 독립적인 스레드에서 모든 레스토랑에 대한 데이터 추출을 수행했다. t =Thread(target=scrape,args=(url[i],))를 사용하여 새로운 스레드를 생성하고 t.join () 호출로 각각의 결과를 얻었다.

Page 50.

이게다야! Ryan은 이 노력에 매우 만족한다. 이 예제에서 Ryan을 도왔고 그를 위해 중요한 비즈니스 업무를 자동화했다. 이 책에서는 파이썬이 비즈니스 프로세스를 자동화하고 효율적으로 활용할 수 있는 다양한 유스 케이스를 살펴볼 것이다. 더 많은 것에 관심이 있나? 이제, 다음 장을 살펴보자.

Page 51.

2. CSV 및 Excel 워크시트 작업

중요한 문서가 파일에 저장되고 업무용 책상에서 관리되는 세계를 상상해본다. 엑셀 시트와 같은 컴퓨터 및 소프트웨어의 출현으로 우리는 체계적인 방식으로 데이터를 관리할 수 있다. 실제로 자동화된 방법으로 워크시트를 관리할 수 있으며 파이썬에서도 워크시트를 관리할 수 있다.

2장에서는 다음과 같은 레시피를 다룰 것이다.

-reader 객체로 CSV 파일 읽기
-CSV 파일에 데이터 쓰기
-나만의 CSV 방언(dialects) 개발
-자동화된 방식으로 직원 정보 관리
-Excel 시트 읽기
-워크시트에 데이터 쓰기
-Excel 셀(cell) 포맷
-Excel 수식으로 재생하기
-Excel 시트 내 차트 작성
-회사 재무 비교 자동화

Page 52.

소개

컴퓨터가 일상 생활의 일부가 될 때까지 사무실 기록은 종이로 만들고 캐비닛에 보관됐다. 오늘날 날로 증가하는 전산 분야(computational field) 덕분에, 컴퓨터 애플리케이션을 사용한 이러한 기록을 텍스트 파일에 저장한다. 텍스트(.txt) 파일은 많은 양의 데이터를 저장하는 데 유용했다. 텍스트 파일 내에서 정보를 쉽게 검색할 수 있었지만 데이터는 체계적 방법으로 저장되지 않았다. 정보가 증가함에 따라 정보를 저장해야 할 필요성도 커졌고 결과적으로 데이터가 구조화된 형식으로 저장될 뿐만 아니라 읽기 및 처리가 쉬워진 CSV 및 Excel 시트가 생겨났다.

CSV 파일에는 콤마(,)로 구분된 데이터가 포함된다. 따라서 CSV(comma separated values)파일이라고 한다. CSV는 표 형식으로 데이터를 저장할 수 있다. CSV 파일은 사용중인 소프트웨어와 독립적으로 모든 스토리지 시스템에서 임포트가 더 쉽다. CSV 파일은 평문 텍스트 파일이므로 쉽게 수정할 수 있고 빠른 데이터 교환에 사용된다.

반면 엑셀 시트에는 탭 혹은 다른 구분기호로 구분된 데이터가 포함된다. 엑셀 시트는 열과 행의 그리드 형태로 데이터를 저장하고 검색한다. 수식을 사용하여 데이터 서식을 지정할 수 있으며 파일에 여러 시트를 오픈할 수 있다. 엑셀은 판매 수치 혹은 커미션과 같은 회사 데이터를 입력, 계산 및 분석하는 데 이상적이다.

CSV 파일은 프로그램에서 데이터를 저장 및 검색하는 데 사용되는 텍스트 파일이지만 엑셀 파일은 이진 파일(binary files)이며 차트 작성, 계산 및 보고서 저장과 같은 좀 더 고급 작업에 사용된다.

파이썬에는 CSV 및 엑셀 파일 모두에서 사용할 수 있는 유용한 모듈이 있다. CSV 및 엑셀 파일을 읽고 쓸 수 있고, 엑셀 셀(cell)을 포맷하고, 차트를 준비하고, 수식을 사용하여 데이터를 계산할 수 있다.

이 장의 레시피는 CSV 및 엑셀 시트에서 이전 작업을 수행하는 데 도움이 되는 파이썬 모듈을 살펴볼 것이다. 특히 2장의 다음 파이썬 모듈을 다룰 것이다.

<주소>

Page 53.

reader 객체로 CSV 파일 읽기

이 레시피에서는 CSV 파일을 읽는 방법, 특히 reader 객체를 생성하고 사용하는 방법을 다룬다.

준비 하기

이 레시피를 실행하려면 파이썬 v2.7을 설치해야한다. CSV 파일을 작업하기 위해, 기본 Python 설치 패키지로 제공되는 좋은 모듈인 csv가 있다. 이제 시작하자.

시작 하기

1. Linux/Mac 컴퓨터에서 터미널로 이동하여 Vim을 사용하거나 원하는 편집기를 선택한다.
2. 우선 CSV 파일을 생성한다. 알다시피 CSV 파일은 데이터가 콤마로 구분된 구조화된 형식이므로 CSV를 생성하는 것은 쉽다. 다음 스크린샷은 해당 국가의 여러 담당자의 세부 정보가 포함된 CSV 파일이다. mylist.csv로 이름을 지정한다.

<그림>

3. 이제 이 CSV 파일을 읽기 위한 파이썬 코드를 작성하여 그것으로 데이터를 출력한다.

<소스>

Page 54.

앞의 코드 조각의 출력은 다음과 같다.

<그림>

4. 오! 어떻게 된 거지? 실수를 범한 것 같다. 이 오류는 CSV reader가 줄 바꿈 문자를 찾지 못했음을 나타낸다. Mac 플랫폼에서 작성된 CSV 파일에서 발생한다. Mac OS가 라인 문자의 끝으로 CR(carriage return)을 사용하기 때문이다.
5. 파이썬에는 이 문제에 대한 간단한 해결책이 있다. rU 모드(universal newline 모드)에서 파일을 연다. 다음 프로그램은 완벽하게 실행되며 파일 컨텐츠를 적절하게 읽을 수 있다.

<소스>

앞의 프로그램의 출력은 다음과 같다.

<그림>

6. 멋지다! 이전 코드 조각에서 살펴본 문제에 대한 또 다른 간단한 해결책이 있다. 우리가 할 수 있는 일은 단순히 파일 형식을 맥(Mac) CSV에서 윈도우즈(Windows) CSV로 변경하는 것이다. 파일에 대해 Open 및 Save As 작업을 수행하여 이 작업을 수행할 수 있다. 다음 예제에서는 mylist_wavsv.csv(Windows CSV 형식)로 mylist.csv를 저장했으며 파일 컨텐츠를 읽는 것이 더 이상 문제가되지 않는다.

<소스>

Page 55.

앞의 코드 예제에서는 CSV 파일에서 데이터의 일부를 출력한다. 알고 있다면 CSV 파일은 첫 번째 색인을 행으로, 두 번째 색인을 열로하여 파이썬에서 2D 목록으로 읽을 수 있다. 여기서 row1과 row2의 두 번째, 세 번째 및 네 번째 열을 출력한다.

<그림>

파이썬에서는 유용한 DictReader(f) 메소드를 사용하여 딕셔너리에 있는 CSV 파일의 컨텐츠를 읽는 것이 매우 편리하다.

<소스>

앞의 코드 조각에서 파일 핸들(file handle) f로 파일을 연다. 이 파일 핸들은 DictReader()에 대한 인자로 사용되며 첫 번째 행 값을 열 이름으로 처리한다. 이러한 열 이름은 데이터가 저장되는 딕셔너리의 키 역할을 한다. 따라서 앞의 프로그램에서는 세 개의 열에서 선택적으로 데이터를 출력할 수 있다. first_name, last_name 및 e-mail을 검색하여 다음 스크린샷과 같이 출력한다.

<그림>

csv 모듈의 DictReader()는 CSV 파일을 읽기 쉽게 만드는 몇 가지 헬퍼 메소드(helper methods) 및 속성(attributes)이 있다. reader objects라고도 알려져있다.

<소스>

Page 56.

2017.04.10

<소스>

이 코드 예제에서는 다음 속성(attributes)과 메소드(method)를 사용했다.

-fieldnames : 열 이름 목록을 제공한다.
-dialect: CSV 파일 형식(자세한 내용을 다룰 것이다.)
-line_num: 읽고있는  현재 행 번호
-next(): 다음 줄로 이동한다.

다음 스크린샷에서 첫 번째 줄에는 CSV 파일의 모든 열 이름이 포함된다. 두 번째 줄에서는 CSV 파일을 읽는 데 사용된 방언(dialect)을 출력한다. 세 번째 행은 현재 읽고 있는 행 번호를 출력하고 스크린 샷의 마지막 행은 읽는 동안 reader 객체가 이동할 다음 행을 나타낸다.

<그림>

There's more

파이썬 모듈 csv는 헬퍼(helper)이며 open() 메소드로 파일을 열고 readline() 메소드로 파일 내용을 읽음으로써 CSV 파일을 완벽하게 처리할 수 있다. 그런 다음 파일의 모든 행에서 split() 작업을 수행하여 파일 내용을 가져올 수 있다.

리딩(Reading)은 훌륭하지만, 무엇인가 CSV 파일에 쓰여질 때만 읽을 것이다. 그렇지?(확인) 다음 레시피에서 CSV 파일에 데이터를 쓰는 데 사용할 수 있는 메소드를 살펴보자.

CSV 파일에 데이터 쓰기

다시 말해서, 이 절의 레시피에서는 파이썬 설치와 함께 번들로 제공되는 모듈, 즉 csv 모듈과 별도로 새로운 모듈이 필요하지 않다.

Page 57.

2017.04.13

작동원리

1. 우선, 쓰기 모드(write mode)와 텍스트 형식(text format)으로 파일을 열 수 있다. CSV 파일에 쓸 데이터를 포함하는 두 개의 파이썬 목록을 생성한다. 다음 코드는 이러한 작업을 수행한다.

<소스>

2. 다음 write() 메소드를 사용하여 CSV 파일에 데이터를 추가하자.

<소스>

3. 앞의 코드에서 우리는 CSV 파일을 헤더로 초기화했다. 사용된 열 이름은 Sr., Names 및 Grades이다. 다음으로 파이썬 for 루프를 4회 실행하고 CSV 파일에 4행의 데이터를 쓴다. 파이썬 목록인 Names, Grades으로 데이터를 가지고 있음을 기억한다. writerow() 메소드는 실제로 CSV 파일에 내용을 추가하고 for 루프 안에 하나씩 행을 추가한다.

앞의 코드 조각의 출력은 다음 스크린샷에서 확인할 수 있다.

<그림>

Page 58.

멋지다, 간단하고 올바르다. 흥미롭게도 기본적으로 CSV 파일에 쓸 때 행의 파일 내용은 콤마로 구분된다. 그러나 만약 탭(\ t)으로 분리되도록 행동을 바꾸고 싶다면 어떻게 해야하나? writer() 메소드는 구분 기호뿐만 아니라 라인 종결자도 변경하는 이 기능을 제공한다.(주의:구분 기호는 CSV 파일에서 한 행의 데이터를 구분하는 데 사용되는 문자이다. 종료 문자는 CSV 파일의 끝행을 표시하는 데 사용된다. 다음 예제에서는 이와 관련된다.)

<소스>

앞의 코드 조각을 실행하면 파일시스템에 새 파일인 write.csv가 생성된다. 파일 내용은 다음 스크린샷에서 확인할 수 있다. 주어진 행의 내용을 보면 콤마가 아닌 탭으로 구분된 것을 확인할 수 있다. 새로운 행 구분 기호는 리턴 키(두 번 눌러진)이며 다음 스크린샷에서도 분명하다. 두 행 간에는 추가 뉴라인 문자(extra newline character)가 있다.

<그림>

자신의 CSV 방언(dialects) 개발

CSV 파일을 읽고 쓰는 것을 더 쉽게하기 위해 csv 모듈의 Dialect 클래스의 일부인 형식 매개변수를 지정할 수 있다. 여기에서는 사용 가능한 방언 중 일부를 살펴보고 작성하는 방법을 살펴본다.

준비하기

이 레시피에서는 파이썬의 기본 설치에 있는 것과 동일한 csv 모듈을 사용하므로 명시적으로 설치할 필요가 없다. 

Page 59.

2017.04.17

실행방법

1. 우선 Dialect 클래스에 있는 몇 가지 속성을 살펴보자.
-Dialect.delimeter: CSV 파일의 행에 내용이 쓰여지는 방식을 변경한 이전 레서피에서 이것을 사용했다. 두 필드를 구분하는 데 사용된다.
-Dialect.lineterminator: CSV 파일에 추가된 행의 종료를 나타내는 데 사용된다. 이전 절에서도 이 방법을 사용했다.
-Dialect.skipinitialspace:  구분 기호(delimiter) 뒤에 모든 선행 공백을 건너뛴다. 우발적인 사람의 실수를 피하는 데 도움이 된다.

다음 코드를 사용하여 사용 가능한 방언(dialects) 목록을 얻을 수 있다.

<소스>

1. 사용 가능한 두 가지 주요 dialects는 excel과 excel-tab이다. Excel dialect는 Microsoft Excel의 기본 내보내기 형식으로 작업하는 데 사용되며 OpenOffice 혹은 NeoOffice에서도 작동한다.

2. 우리가 선택한 dialect를 생성하자. 예를 들어, CSV 파일의 열을 구분하기 위해 - 기호를 선택한다.

<소스>

다음과 같은 파일인 pipes.csv를 생성한다.

<그림>

Page 60.

위의 파이썬 코드를 pipes.csv 파일에서 실행하면 모든 행을 - 문자로 분리된 모든 요소가 있는 배열로 반환한다. 다음 스크린샷은 프로그램의 결과를 보여준다.

<그림>

작동원리

두 번째 코드 조각에서는 register_dialect() 메소드를 사용하여 자체 dialect를 등록한다. 우리는 우리의 dialect을 pipes로 지었고 pipes와 관련된 구분은 의도한데로 - 기호이다.

이제 자신의 read() 메소드를 사용하여 pipes.csv 파일을 읽고 reader 객체를 사용하여 CSV 파일의 내용을 가져온다. 그러나 기다려. dialect='pipes'의 사용을 보았나? reader가 열을 -로 구분하여 읽을 것을 기대하고 그에 따라 데이터를 읽는 것을 확신한다.

만약 당신이 관찰한다면, reader 객체는 방언(dialect) pipes로 정의된 -에 기반한 행을 분할한다.

자신의 데이터를 CSV 파일로 읽고 쓰는 방법을 배웠다. 당신은 또한 방언의 사용법을 이해했다. 앞의 개념을 실제 사용 사례와 함께 사용하는 방법에 대한 느낌을 얻는다.

자동화된 방식으로 직원 정보 관리

Mike는 조직의 HR 관리자이며 캘리포니아 주에서 모든 직원의 연락처 정보를 수집하려고 한다. 그는 CA 주의 모든 직원에 대한 설문 조사를 수행할 수 있도록 이 정보를 분리하려고 한다. 그는 이 정보를 수집하기를 원할뿐만 아니라 다른 CSV 파일에 저장하여 나중에 작업하기 쉽다.(확인)

여기서 Mike를 도울 수 있을까? 지금까지 배운 개념을 어떻게 적용할까? Mike를 도우면서 더 많은 것을 배울 수 있을까? 구현을 살펴보자.

Page 61.

시작하기

이 예제에서는 특별한 모듈이 필요없다. 이전 레시피의 일부로 설치된 모든 모듈은 우리에게 충분하다. 이 예제에서는 직원 정보가 들어있는 동일한 mylist.csv 파일을 사용한다.

실행방법

1. 코드에 직접 들어가서 두 개의 파일을 열자. 하나의 파일 핸들은 파일 내용(직원 데이터 읽기)을 읽는 데 사용되며 다른 파일 핸들은 CA_Employees.csv 파일에 쓰는 데 사용된다. 파일이 열리는 모드의 차이에 유의한다('rt'및 'wt'). 물론 직원 CSV 파일은 읽기 모드로 열리고 CA_Employees.csv 파일은 쓰기 모드로 열린다.

<소스>

2. 다음으로 DictReader() 메소드를 사용한 딕셔너리로 CSV 파일의 직원 정보를 읽는다. 또한 CA_Employees.csv 파일에 데이터를 쓰는 csvWriter 객체를 생성한다.

3. CSV 파일의 행을 읽기 시작할 때 첫 번째 행을 읽는 것을 추측한다. 이 행에는 열 이름만 포함되어 있으므로 건너뛴다. 맞을까?(확인) 예, reader 객체의 line_num 속성을 사용하여 헤더를 건너뛴다(이 장의 앞부분에서 설명한 속성을 살펴본다). 헤더를 건너뛰면, 모든 행은 반복하고 CA 주에 속한 직원을 필터링하고 이러한 직원에 대한 e-mail 및 전화 정보를 가져온다. 필터링된 데이터는 csvWriter 객체를 사용하여 CA_Employees.csv 파일에 쓰여진다. 파일 작업이 완료되면 메모리 누수 혹은 데이터 불일치가 발생할 수 있으므로 파일 핸들을 닫는 것이 중요하다.

<소스>

Page 62.

<소스>

작동원리

위의 프로그램을 모두 실행하면 다음 스크린샷과 같은 CA_Employees.csv 파일이 생성된다.

<그림>

코드 구현을 살펴보면 line_num 속성을 사용하여 mylist.csv 파일의 첫 번째 행인 헤더 행을 건너뛴다. writerow() 메소드를 사용하여 필터링된 데이터를 새로 생성한 CA_Employees.csv 파일에 쓴다. 잘 했어, 마이크(Mike)가 너에게 행복하다고 생각해. 그의 문제가 해결됐다.

CSV 파일 작업에 대해서는 이 절의 끝으로 넘어간다. CSV 파일은 본질적으로 순수 텍스트 형식의 저장 데이터를 저장한다. 이 파일들로 많은 것을 성취할 수 없으므로 엑셀 시트가 출현한다. 다음 레시피에서는 엑셀 시트 작업을 시작하고 그들이 제공 할 수 있는 것에 감사한다.

Excel 시트 읽기

아시다시피 Microsoft Office는 Office 2007의 .xlsx인 Microsoft Excel 시트에 대한 새로운 확장을 제공하기 시작했다. 이 변경으로 Excel 시트가 ZIP 압축을 사용하여 XML 기반 파일 형식(Office Open XML)으로 이동했다. Microsoft는 비즈니스 커뮤니티가 그들을 푸시하는 애플리케이션 간에 데이터를 전송하는 데 도움이 되는 열린 파일 형식을 요구할 때 이 변경 작업을 수행했다. 파이썬으로 Excel 시트를 사용하여 작업하는 방법을 살펴보자.

Page 63.

시작하기

이 레시피에서는 openpyxl 모듈을 사용하여 Excel 시트를 읽는다. openpyxl 모듈은 Excel 시트에서 읽기 및 쓰기 작업을 모두 수행하는 포괄적인 모듈이다. openpyxl에 대한 또 다른 대안은 xlrd 모듈이다. xlrd는 1995년 이래로 Excel 형식을 지원해 왔지만 이 모듈은 Excel 시트에서 데이터를 읽는 데만 사용할 수 있다. openpyxl 모듈은 Excel 파일 작업에 필수적인 데이터 수정, 데이터 쓰기 및 복사와 같은 더 많은 작업을 수행하는 데 도움이 된다.

openpyxl 모듈을 가장 좋아하는 도구인 pip로 설치해보자.

<소스>

작동원리

1. 먼저 스크린샷과 같은 내용의 Excel 시트를 생성한다. Excel 파일은 workbooks라고하며 하나 이상의 워크시트(worksheets)를 포함하므로 Excel 파일을 스프레드시트(spreadsheets)라고도 한다. myxlsx.xlsx라는 파일로 People 및 Items라는 두 개의 시트에 저장한다. 

People 시트의 데이터를 살펴보자.

<그림>

Page 64.

2017.04.18

이제 Items 시트의 데이터를 살펴보자.

<그림>

2. 이제 XLSX 파일을 읽어보자. 다음 코드는 Excel workbook에 있는 모든 워크시트(worksheets)의 이름을 가져오는 데 도움이 된다.

<소스>

3. 이제 주어진 시트 작업을 원한다면 그 객체에 접속하는 방법은 무엇인가? 다음 코드 조각은 People 워크시트로 이동한다.

<소스>

와우, 멋지다.

4. 이제 이동하여 셀 객체를 읽어보자. 이름 혹은 행/열 위치를 기준으로 셀을 읽을 수 있다. 다음 코드 조각은 이를 보여준다.

<소스>

Page 65.

5. 그러나 셀의 값을 어떻게 얻을 수 있을까? 간단히 말해서, object.value는 셀에 있는 값을 반환한다.

<소스>

파이썬 코드 조각을 실행하면 이 스크린샷에서 볼 수 있듯이 다음과 같은 결과가 나타날 것이다.

<그림>

작동원리

앞의 예제에서는 openpyxl 모듈을 임포트한다. 이 모듈에는 worksheet 객체와 셀에 접속할 수 있는 메소드가 있다. load_workbook() 메소드는 메모리에 전체 Excel 시트를 로드한다. get_sheet_names() 및 get_sheet_by_name() 메소드는 해당 workbook의 워크시트를 선택하는 데 도움이 된다. 따라서 workbook과 워크시트 객체를 준비할 수 있다.

셀 객체는 cell() 메소드를 사용하여 접속할 수 있으며 cell().value는 worksheet의 셀에 있는 실제 값을 반환한다. 좋아. 파이썬으로 Excel 시트에서 데이터를 읽는 것이 얼마나 쉬운 지 살펴보자. 그러나 reading은 Excel 시트에 데이터를 쓰는 방법을 알고있는 경우에만 유용하다. 그래서, 무엇을 기다리고 있나? 계속해서 다음 레시피에서 배우자.

워크시트(worksheets)에 데이터 쓰기

파일 읽기는 openpyxl 모듈을 사용하면 편리하다. 이제 엑셀 파일 작성을 살펴보자. 이 절에서는 Excel 파일로 여러 작업을 수행한다.

Page 66.

시작하기

이 레시피에서는 xlsxwriter인 또다른 환상적인 파이썬 모듈을 사용할 것이다. 이름에서 알 수 있듯이, 이 모듈은 Excel 시트에서 여러 작업을 수행하는 데 도움이 된다. 흥미롭게도 xlsxwriter는 Excel 시트의 읽기 작업을 지원하지 않는다(이 책을 쓰는 시점). 다음과 같이 pip를 사용하여 xlsxwrite 모듈을 설치한다.

<소스>

작동원리


