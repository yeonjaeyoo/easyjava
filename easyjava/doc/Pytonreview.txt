시작문자는 0 혹은 많은(\ w *), 종료 문자는 (0 또는 여러 문자 (\ w *) 혹은 동일한 문자가 뒤에 오는 문자(\ w))와 일치한다.
분할 과정이 계속되고, 최종적으로 얻은 결과 문자열은 lot이다.
이 문제를 피하기 위해 wordnet을 그것과 함께 임베드(embed) 할 수 있다.
단어를 동의어로 대체
단어를 동의어로 대체 예제
텍스트에 지프의 법칙(Zipf's law) 적용
지프의 법칙에 따르면 텍스트의 토큰 빈도는 정렬된 목록의 순위(rank) 혹은 위치(position)에 정비례한다.
이 법칙은 토큰이 언어로 배포되는 방법을 설명한다.
일부 토큰은 매우 자주 발생하며 일부는 중간 빈도로 발생하며 일부 토큰은 거의 발생하지 않는다.
위의 코드는 문서의 단어 빈도 대 순위 빈도를 구한다.(확인)
따라서 지프의 법칙이 순위와 단어의 빈도 간의 비례 관계를 봄으로써 모든 문서에 대해 유지되는지 여부를 확인할 수 있다.

Similarity measure(유사성 척도)
NLP 작업을 수행하는 데 사용할 수 있는 많은 유사성 척도가 있다.NLTK의 nltk.metrics 패키지는 다양한 NLP 작업을 수행하는 데 도움이 되는 다양한 평가 혹은 유사성 측도를 제공하는 데 사용된다.
훈련 파일(training file)에서 얻은 표준 점수를 사용한 개체명 인식기(named entity recognizer)의 출력을 분석하는 방법을 살펴보자.

2017.01.18

기본적으로 선택되지 않음여기에서 relate와 relation 간의 편집 거리를 계산할 때 세 가지 작업(1번의 치환 작업과 2번의 삽입 작업)이 수행된다.

suggestion과 calculation 간의 편집 거리를 계산하는 동안 7가지 작업(6번의 치환 작업과 1번의 삽입 작업)이 수행된다.

자카드 계수(Jaccard's coefficient) 혹은 타니모토 계수(Tanimoto coefficient)는 X와 Y의 두 세트의 오버랩 측정으로 정의될 수 있다.

2장

컴퓨터 언어학은 해석학, 소프트웨어 애플리케이션 및 사람들이 머신과 통신하는 상황에서 널리 사용되는 새로운 영역이다.
자연 언어 텍스트에서 수행할 수 있는 전처리 작업 혹은 계산을 이해하는 것이 중요하다.
여기에서 바이그램의 빈도를 10에서 다른 숫자로 변경할 수 있다.

2017.01.20

fourgrams을 생성하고 fourgrams의 빈도를 생성하려면 다음 코드가 사용된다.
지수 분류기라고도 하는
이 모듈에서는 모든 확률 분포가 훈련 데이터에 따라 고려된다.
이 모델은 두 가지 기능, 즉 입력 기능 및 결합 기능을 참조하는 데 사용된다.
입력 기능은 라벨이 없는 단어의 기능이라고 할 수 있다.
MLE는 텍스트에서 주어진 발생에 대한 확률 분포를 포함하는 freqdist를 생성하는 데 사용된다.

2017.01.24
observed state : 관찰이 가능한 상태
latent states : 은닉 상태
HMM 추정을 사용하여 테스트를 수행할 수 있다.
우리는 추정 법칙(estimator)을 테스트했다.
에드온 스무딩
1 대신에 알려지지 않은 단어의 개수에 다른 값을 더할 수 있으므로 알려지지 않은 단어를 처리할 수 있고 그 확률은 0이 아니다.
가짜 수(Pseudo count)는 알려지지 않은 단어의 수에 추가되어 확률이 0이 아닌 값(즉 1 혹은 0이 아닌 값)이다.
각 단어의 수를 정규화해야 한다.
Good Turing이 방법은 보이지 않는 객체의 확률을 예측하는 데 도움이 된다.
이 방법에서는 이항 분포(binomial distribution)가 우리의 관심 객체에 의해 나타난다.

Simple Good Turing은 선형 회귀(linear regression)에 의해 로그 공간에서 선형 라인(linear line)으로 빈도에서 빈도까지 근사치를 수행한다.

c == 0 인 경우 훈련에서 0의 빈도를 갖는 샘플 = N(1)여기서 c는 원래의 카운트이고 N(i)는 카운트 i에서 관찰된 이벤트 타입의 수이다.
한 쌍(pi, qi)이 주어진다면 pi는 빈도를 나타내고 qi는 빈도의 빈도를 의미하며, 우리의 목표는 제곱 변화를 최소화하는 것이다.
E(p)와 E(q)는 pi와 qi의 평균이다.
슬로프(slope)
인터셉트(intercept)
param freqdist는 확률 분포가 추정되는 빈도 수를 나타낸다.
Param bin은 가능한 샘플 수를 추정하는 데 사용된다.
Nr(r)> 0 인 2개의 목록(r, Nr)에서 도수 분포를 나눈다.
count 및 Nr(count)에 따라 로그-로그 공간(log-log space)에서 self._slope 및 self._intercept 매개변수를 조정하기 위해 간단한 선형 회귀 분석을 사용한다(로그 공간에서 작업하여 부동 소수점 언더플로우(floating point underflow)가 발생하지 않도록한다.).
더 높은 샘플 빈도의 경우 데이터 포인트는 라인 Nr=1을 따라 수평이 된다.
로그-로그 공간(log-log space)에서 더 명확한 선형 모델을 생성하기 위해, 우리는 주변 0 값으로 양의 Nr 값을 평균화한다.(처지와 게일(Church and Gale), 1991)
r 혹은 nr이 비었나?
E[Nr]를 추정할 경우 Nr에서 Sr로 전환해야하는 r 프론티어(frontier)를 계산한다.
r의 끝에 있거나 혹은 r에 간격이 있다.
카운트 r로 샘플의 수를 반환한다.
Nr = a*r^b(적절한 쌍곡선형 상관(hyperbolic relationship)을 제공하기 위해 b < -1)방정식의 대수 형태(logarithmic form)에 대한 간단한 선형 회귀 기법으로 a와 b를 추정하라.
방정식의 로그 형태에 대한 간단한 선형 회귀 기법으로 a와 b를 추정한다(log Nr = a + b * log (r)).
샘플의 확률을 반환한다.
assert prob_sum != 1.0, "확률 합은 하나여야 한다!"
그것은 보이는 이벤트에서 보이지 않는 이벤트로 전체 확률 전송을 제공하는 데 사용된다.
ProbDist의 문자열 표현을 얻는다.
크네저 네이(Kneser Ney)
위튼 벨은 확률이 0인 알려지지 않은 단어를 다루기 위해 고안된 스무딩 알고리즘이다.
이 모델에 따르면 훈련에서 n 그램(n gram)이 n번 이상 보일 경우 이전 정보가 주어지면 토큰의 조건부 확률은 그 n 그램의 MLE에 비례한다.
그렇지 않으면 조건부 확률은 (n-1) 그램의 백오프 조건부 확률(back-off conditional probability)과 동일하다.
이 문맥에서는 이 단어의 확률을 Katz Backoff를 사용하여 평가한다.
param word : 확률을 구하는 단어
param context : 단어가 있는 문맥

추가된 스무딩 바이그램을 사용하는 한계는 희귀 텍스트를 처리할 경우 무지의 상태로 되돌아가는 것이다.
우리는 unigram과 bigram 확률 모두를 결합할 수 있는 보간 모델을 개발할 수 있다.
SRILM에서, 우리는 bigram 모델에 사용 된 -order 1과 -order 2를 가진 unigram 모델을 먼저 훈련함으로써 보간을 수행한다.
perplexity 혼잡도
주어진 텍스트의 혼잡도를 계산한다.
이것은 단순히 텍스트에 대한 2** 교차-엔트로피(2 ** cross-entropy)이다.
: param text : 혼잡도를 계산할 단어
머리의 개수와 동전의 던지기의 수를 특징으로하는 두 개의 동전을 던지는 예제를 생각해본다.

2017.01.25

준접사는 예를 들어 주목할만한, 반사회적, 반 시계 방향, 품질 등의 단어를 나타내는 형태소를 바인딩한다.
정보 검색의 정확성을 높이기 위해 검색 엔진은 주로 스테밍을 사용하여 어간을 얻고 색인어(indexed words)로 저장한다.
검색 엔진은 같은 의미의 동의어를 가진 단어를 호출하고 이는 동의어로 알려진 일종의 검색어 확장일 수 있다.

2017.01.31

검색엔진
그것은, 구성된다.
문맥 관점에서 자주 발생하고 중요하지 않은 단어를 제거한다.
키워드를 벡터 차원(dimensions)에 매핑하는 다음 코드를 살펴본다.
문서 벡터에서 요소의 주어진 위치에 대한 키워드를 생성한다.
텍스트를 문자열로 매핑한다.
검색의 중요성이 없는 일반적인 단어를 제거한다.
이 토큰을 설명하는 데 사용되는 차원과의 매핑을 수행하는 키워드(keywords)에 포지션(position)을 연결한다.
텍스트 문자열을이것은 문서가 관련성이 있다고한다.
이것은 문서가 관련성이 없다고한다.
5.키워드와 벡터 공간의 매핑을 수행한다.
검색할 항목을 나타내는 임시 텍스트를 구성한 다음 코사인 측정을 통해 문서 벡터와 비교한다.
항목 목록을 기반으로 일치하는 텍스트 검색
이제 소스 텍스트에서 언어를 탐지하는 데 사용할 수 있는 다음 코드를 살펴볼 것이다.
다른 언어로 작성될 수 있는 주어진 문서의 확률을 계산하고 {'german': 2, 'french': 4, 'english': 1}와 유사한 딕셔더리를 제공한다.
nltk.wordpunct_tokenize()는 모든 문장 부호를 별도의 토큰으로 분할한다.
앞의 코드는 불용어를 검색하고 텍스트 언어, 즉 영어를 감지한다.
스테밍, 원형복원(lemmatization) 및 형태소 분석 및 생성, 그리고 NLTK에서 구현을 살펴봤다.

2017.02.01

앞의 코드는 동사구의 모든 태그에 관한 정보를 제공한다.
POS 태깅을 통해 얻어지는 단어의 모호성을 보여주는 예제를 살펴보자.
주어진 텍스트에서 튜플 시퀀스가 생성될 수 있는 예제를 살펴보자.
명사 태그 전에 나오는 태그의 수를 계산하는 다음 코드를 살펴보자.
기본 태깅은 모든 토큰에 동일한 품사 태그를 할당하는 일종의 태깅이다.
choose_tag() 메소드는 SequentialBackoffTagger에 의해 구현되야한다.
이제 DefaultTagger의 작업을 설명하는 다음 코드를 살펴보자.
nltk.tag.untag()를 사용하여 태그가 있는 문장을 태그가 없는 문장으로 변환할 수 있다.
NLTK에서 제공하는 POS 태거(tagger)를 사용하여 토큰 목록에 태그를 지정할 수 있다.
NLTK에 의해 제공된 품사 태거(part of speech tagger)를 사용하여 토큰 목록의 태깅을 수행할  수 있다.

2017.02.02
POS 태깅은 단어 범주 모호성 제거 혹은 문법 태깅이라고도한다.
규칙 기반(rule-based) 혹은 확률적(stochastic)/확률적(probabilistic)
POS 분류기(POS classifier)는 문서를 입력으로 가져오고 단어 기능을 얻는다.
이러한 유형의 분류기를 2차 분류기(second order classifier)라고하며, 단어의 태그를 생성하기 위해 부트스트랩 분류기(bootstrap classifier)를 사용한다.
백오프 분류기는 백오프 절차가 수행되는 분류기이다.
출력은 trigram POS tagger가 tigram POS tagger에 의존하는 방식으로 얻어지며, POS tagger는 unigram POS tagger에 의존한다.
학습=>훈련
사전=>딕셔너리
주변 단어 및 태그지도 분류에서, 트레이닝 코퍼스는 단어와 그 정확한 태그를 포함하는 데 사용된다.
<그림 정의>
지도 분류에서, 훈련 도중, 특징 추출기(feature extractor)는 입력과 라벨을 받아들이고 일련의 특징을 생성한다.
라벨과 함께 설정된 이 특징(features)? 기능?은 기계 학습 알고리즘의 입력으로 사용된다.
classifier:분류기
테스트 혹은 예측 단계에서 알 수 없는 입력으로 특징을 생성하는 특징 추출기가 사용되며 출력은 기계 학습 알고리즘을 사용하여 라벨 또는 pos 태그 정보의 형태로 출력을 생성하는 분류기 모델(classifier model)로 전송된다.
최대 엔트로피 분류기는 훈련에 사용된 코퍼스의 전체 가능성을 극대화하기 위해 매개변수 집합을 검색하는 것이다.

2017.02.03
UnigramTagger의 훈련은 초기화시 문장 목록을 제공하여 수행할 수 있다.
앞의 코드에서 트리뱅크(Treebank) 코퍼스의 첫 번째 7,000개의 문장을 사용하여 훈련을 수행했다.
UnigramTagger가 따르는 계층 구조는 다음 상속 다이어그램에 설명된다.
따라서 그것은 pos 태깅을 올바르게 수행하는 데 96% 정확하다.UnigramTagger는 ContextTagger에서 상속되므로 컨텍스트 키(context key)를 특정 태그와 매핑할 수 있다.
주어진 문맥에서 ContextTagger는 주어진 태그의 빈도를 사용하여 가장 가능성있는 태그의 발생을 결정한다.
어떤 태거가 단어에 태그를 지정할 수 없는 경우 다음 태거를 사용하여 태그를 지정할 수 있다.
BigramTagger는 이전 태그를 문맥 정보로 사용한다.
NgramTagger를 사용하여 3보다 큰 n에 대한 태거를 생성할 수도 있다.

2017.02.06
구문 분석이라고도하는 파싱(Parsing)은 NLP의 작업 중 하나이다.
파싱은 자연 언어로 쓰여진 문자 순서가 형식 문법(formal grammar)으로 정의된 규칙에 부합하는지 여부를 찾는 과정으로 정의된다.
문장을 단어 혹은 구문 시퀀스(phrase sequences)로 분해하고 특정 구성 요소 범주(명사, 동사, 전치사 등)를 제공하는 과정이다.
그래서 파스 트리의 생성에 이어 의미를 추가한다.
morphological trees:형태론적 계도
이 과정에서 몇 가지 결과가 얻어지면 중간 결과로 간주되어 향후 결과를 얻기 위해 재사용될 수 있다.

2017.02.07

양식(form)의 생성 규칙 집합(P)
A→a, 여기서 AÎN과 a는 단말과 비 단말로 구성된다.
CFG의 문장 수준 구조에는 네 가지 구조가 있다.

2017.02.15
선언적 구조(Declarative structure)
declarative sentence(서술문)
명령형 구조(Imperative structure)
명령문, 명령(commands) 혹은 제안(commands)(명령은 동사구로 시작하고 주어가 포함되지 않음)을 처리한다.
예/아니오 구조
Wh-질의 구조
일반적인 CFG 규칙은 여기에 요약된다.

2017.02.16

Context-free Grammar:문액 자유 문법
다음과 같이 ATIS에서 테스트 문장을 추출한다.
Bottom-up 필터(filter)를 사용한 Left Corner 파싱
probabilistic Context Free Grammar(확률적 문액 자유 문법)
CFG와 동일한 파스 구조를 생성하지만 각 파스 트리에 확률을 지정한다.
파스된 트리의 확률은 트리를 생성하는 데 사용된 모든 생성 규칙의 확률을 사용하여 구한다.
NLTK에서 PCFG의 규칙 형성을 보여주는 다음 코드를 살펴보자.
Left Recursion:왼쪽 재귀
Earley 알고리즘은 유효하지 않은 파스가 생성될 때 Top-down 예측을 사용한다.
#대각선으로 채운다.
Earley 알고리즘은 1970 년에 Earley가 발표했다.
그것은 왼쪽에서 오른쪽으로 차트를 채운다.
5장에서는 Treebank Corpus에 접속하는 파싱과 문맥 자유 문법(Context-free Grammar), 확률적 문맥 자유 문법(Probabilistic Context-Free Grammar), CYK 알고리즘 및 Earley 알고리즘의 구현에 대해 설명했다.
모호성 제거 작업

2017.02.21
의미 해석은 의미를 문장에 매핑하는 것을 의미한다.
meaning or sense
문장을 분석하고 주어진 입력에 출력을 제공하기 위해 대체(substitution) 및 패턴 일치 기술(pattern matching techniques)을 사용했다.
Robert Schank : 로버트 생크
primitives : 원어
심리 분석 => 의미 분석
similarity retrieval=>유사도 검색? 유사성 검색
감성 분석 => 의미 분석
영어 문장에서 동사, 명사, 형용사, 날짜, 구와 같은 의미 정보를 추출한다.
비가 온다.
나는 비옷을 입을 것이다.
비가 온다면, 나는 비옷을 입을 것이다.
논리 표현식을 다른 서브 클래스로 분류하는 NLTK의 다음 코드를 살펴보자.
명명된 엔티티->개체명
여기에 설명된다.
<표>
순번
개체명 태그
의미
스탠포드 태거

2017.02.22

매개변수(parameter) binary
고려한다.=>살펴본다.
분류자->분류기
교육을 받는다->훈련을 받는다.

2017.02.23
HMM은 최적의 상태 시퀀스를 출력으로 생성한다.
포워드-백워드 알고리즘
방출 -> 출력
posterior marginals(후방 가장자리)

2017.02.24
HMM train에서는 HMM 매개변수인 즉 시작 확률, 전이 확률 및 방출 확률을 계산한다.
HMM test 동안에는 최적의 태그 시퀀스를 찾는 비터비 알고리즘(Viterbi algorithm)이 사용된다.
규칙 기반(Rule-based) 혹은 수제(Handcrafted) 접근
Conditional Random Field : 조건부 임의 필드
서포트벡터머신(Support Vector Machine
)Decision Tree(의사결정 트리)
https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html에서 확인할 수 있다.
Named Entities : 개체명
이러한 토큰은 훈련되지 않아서 여기 일부 토큰은 None 태그로 태그가 지정된다.
주어진 품사 태그로 모든 synset을 반복한다.
