시작문자는 0 혹은 많은(\ w *), 종료 문자는 (0 또는 여러 문자 (\ w *) 혹은 동일한 문자가 뒤에 오는 문자(\ w))와 일치한다.
분할 과정이 계속되고, 최종적으로 얻은 결과 문자열은 lot이다.
이 문제를 피하기 위해 wordnet을 그것과 함께 임베드(embed) 할 수 있다.
단어를 동의어로 대체
단어를 동의어로 대체 예제
텍스트에 지프의 법칙(Zipf's law) 적용
지프의 법칙에 따르면 텍스트의 토큰 빈도는 정렬된 목록의 순위(rank) 혹은 위치(position)에 정비례한다.
이 법칙은 토큰이 언어로 배포되는 방법을 설명한다.
일부 토큰은 매우 자주 발생하며 일부는 중간 빈도로 발생하며 일부 토큰은 거의 발생하지 않는다.
위의 코드는 문서의 단어 빈도 대 순위 빈도를 구한다.(확인)
따라서 지프의 법칙이 순위와 단어의 빈도 간의 비례 관계를 봄으로써 모든 문서에 대해 유지되는지 여부를 확인할 수 있다.

Similarity measure(유사성 척도)
NLP 작업을 수행하는 데 사용할 수 있는 많은 유사성 척도가 있다.NLTK의 nltk.metrics 패키지는 다양한 NLP 작업을 수행하는 데 도움이 되는 다양한 평가 혹은 유사성 측도를 제공하는 데 사용된다.
훈련 파일(training file)에서 얻은 표준 점수를 사용한 개체명 인식기(named entity recognizer)의 출력을 분석하는 방법을 살펴보자.

2017.01.18

기본적으로 선택되지 않음여기에서 relate와 relation 간의 편집 거리를 계산할 때 세 가지 작업(1번의 치환 작업과 2번의 삽입 작업)이 수행된다.

suggestion과 calculation 간의 편집 거리를 계산하는 동안 7가지 작업(6번의 치환 작업과 1번의 삽입 작업)이 수행된다.

자카드 계수(Jaccard's coefficient) 혹은 타니모토 계수(Tanimoto coefficient)는 X와 Y의 두 세트의 오버랩 측정으로 정의될 수 있다.

2장

컴퓨터 언어학은 해석학, 소프트웨어 애플리케이션 및 사람들이 머신과 통신하는 상황에서 널리 사용되는 새로운 영역이다.
자연 언어 텍스트에서 수행할 수 있는 전처리 작업 혹은 계산을 이해하는 것이 중요하다.
여기에서 바이그램의 빈도를 10에서 다른 숫자로 변경할 수 있다.

2017.01.20

fourgrams을 생성하고 fourgrams의 빈도를 생성하려면 다음 코드가 사용된다.
지수 분류기라고도 하는
이 모듈에서는 모든 확률 분포가 훈련 데이터에 따라 고려된다.
이 모델은 두 가지 기능, 즉 입력 기능 및 결합 기능을 참조하는 데 사용된다.
입력 기능은 라벨이 없는 단어의 기능이라고 할 수 있다.
MLE는 텍스트에서 주어진 발생에 대한 확률 분포를 포함하는 freqdist를 생성하는 데 사용된다.

2017.01.24
observed state : 관찰이 가능한 상태
latent states : 은닉 상태
HMM 추정을 사용하여 테스트를 수행할 수 있다.
우리는 추정 법칙(estimator)을 테스트했다.
에드온 스무딩
1 대신에 알려지지 않은 단어의 개수에 다른 값을 더할 수 있으므로 알려지지 않은 단어를 처리할 수 있고 그 확률은 0이 아니다.
가짜 수(Pseudo count)는 알려지지 않은 단어의 수에 추가되어 확률이 0이 아닌 값(즉 1 혹은 0이 아닌 값)이다.
각 단어의 수를 정규화해야 한다.
Good Turing이 방법은 보이지 않는 객체의 확률을 예측하는 데 도움이 된다.
이 방법에서는 이항 분포(binomial distribution)가 우리의 관심 객체에 의해 나타난다.

Simple Good Turing은 선형 회귀(linear regression)에 의해 로그 공간에서 선형 라인(linear line)으로 빈도에서 빈도까지 근사치를 수행한다.

c == 0 인 경우 훈련에서 0의 빈도를 갖는 샘플 = N(1)여기서 c는 원래의 카운트이고 N(i)는 카운트 i에서 관찰된 이벤트 타입의 수이다.
한 쌍(pi, qi)이 주어진다면 pi는 빈도를 나타내고 qi는 빈도의 빈도를 의미하며, 우리의 목표는 제곱 변화를 최소화하는 것이다.
E(p)와 E(q)는 pi와 qi의 평균이다.
슬로프(slope)
인터셉트(intercept)
param freqdist는 확률 분포가 추정되는 빈도 수를 나타낸다.
Param bin은 가능한 샘플 수를 추정하는 데 사용된다.
Nr(r)> 0 인 2개의 목록(r, Nr)에서 도수 분포를 나눈다.
count 및 Nr(count)에 따라 로그-로그 공간(log-log space)에서 self._slope 및 self._intercept 매개변수를 조정하기 위해 간단한 선형 회귀 분석을 사용한다(로그 공간에서 작업하여 부동 소수점 언더플로우(floating point underflow)가 발생하지 않도록한다.).
더 높은 샘플 빈도의 경우 데이터 포인트는 라인 Nr=1을 따라 수평이 된다.
로그-로그 공간(log-log space)에서 더 명확한 선형 모델을 생성하기 위해, 우리는 주변 0 값으로 양의 Nr 값을 평균화한다.(처지와 게일(Church and Gale), 1991)
r 혹은 nr이 비었나?
E[Nr]를 추정할 경우 Nr에서 Sr로 전환해야하는 r 프론티어(frontier)를 계산한다.
r의 끝에 있거나 혹은 r에 간격이 있다.
카운트 r로 샘플의 수를 반환한다.
Nr = a*r^b(적절한 쌍곡선형 상관(hyperbolic relationship)을 제공하기 위해 b < -1)방정식의 대수 형태(logarithmic form)에 대한 간단한 선형 회귀 기법으로 a와 b를 추정하라.
방정식의 로그 형태에 대한 간단한 선형 회귀 기법으로 a와 b를 추정한다(log Nr = a + b * log (r)).
샘플의 확률을 반환한다.
assert prob_sum != 1.0, "확률 합은 하나여야 한다!"
그것은 보이는 이벤트에서 보이지 않는 이벤트로 전체 확률 전송을 제공하는 데 사용된다.
ProbDist의 문자열 표현을 얻는다.
크네저 네이(Kneser Ney)
위튼 벨은 확률이 0인 알려지지 않은 단어를 다루기 위해 고안된 스무딩 알고리즘이다.
이 모델에 따르면 훈련에서 n 그램(n gram)이 n번 이상 보일 경우 이전 정보가 주어지면 토큰의 조건부 확률은 그 n 그램의 MLE에 비례한다.
그렇지 않으면 조건부 확률은 (n-1) 그램의 백오프 조건부 확률(back-off conditional probability)과 동일하다.
이 문맥에서는 이 단어의 확률을 Katz Backoff를 사용하여 평가한다.
param word : 확률을 구하는 단어
param context : 단어가 있는 문맥

추가된 스무딩 바이그램을 사용하는 한계는 희귀 텍스트를 처리할 경우 무지의 상태로 되돌아가는 것이다.
우리는 unigram과 bigram 확률 모두를 결합할 수 있는 보간 모델을 개발할 수 있다.
SRILM에서, 우리는 bigram 모델에 사용 된 -order 1과 -order 2를 가진 unigram 모델을 먼저 훈련함으로써 보간을 수행한다.
perplexity 혼잡도
주어진 텍스트의 혼잡도를 계산한다.
이것은 단순히 텍스트에 대한 2** 교차-엔트로피(2 ** cross-entropy)이다.
: param text : 혼잡도를 계산할 단어
머리의 개수와 동전의 던지기의 수를 특징으로하는 두 개의 동전을 던지는 예제를 생각해본다.

2017.01.25

준접사는 예를 들어 주목할만한, 반사회적, 반 시계 방향, 품질 등의 단어를 나타내는 형태소를 바인딩한다.
정보 검색의 정확성을 높이기 위해 검색 엔진은 주로 스테밍을 사용하여 어간을 얻고 색인어(indexed words)로 저장한다.
검색 엔진은 같은 의미의 동의어를 가진 단어를 호출하고 이는 동의어로 알려진 일종의 검색어 확장일 수 있다.

2017.01.31

검색엔진
그것은, 구성된다.
문맥 관점에서 자주 발생하고 중요하지 않은 단어를 제거한다.
키워드를 벡터 차원(dimensions)에 매핑하는 다음 코드를 살펴본다.
문서 벡터에서 요소의 주어진 위치에 대한 키워드를 생성한다.
텍스트를 문자열로 매핑한다.
검색의 중요성이 없는 일반적인 단어를 제거한다.
이 토큰을 설명하는 데 사용되는 차원과의 매핑을 수행하는 키워드(keywords)에 포지션(position)을 연결한다.
텍스트 문자열을이것은 문서가 관련성이 있다고한다.
이것은 문서가 관련성이 없다고한다.
5.키워드와 벡터 공간의 매핑을 수행한다.
검색할 항목을 나타내는 임시 텍스트를 구성한 다음 코사인 측정을 통해 문서 벡터와 비교한다.
항목 목록을 기반으로 일치하는 텍스트 검색
이제 소스 텍스트에서 언어를 탐지하는 데 사용할 수 있는 다음 코드를 살펴볼 것이다.
다른 언어로 작성될 수 있는 주어진 문서의 확률을 계산하고 {'german': 2, 'french': 4, 'english': 1}와 유사한 딕셔더리를 제공한다.
nltk.wordpunct_tokenize()는 모든 문장 부호를 별도의 토큰으로 분할한다.
앞의 코드는 불용어를 검색하고 텍스트 언어, 즉 영어를 감지한다.
스테밍, 원형복원(lemmatization) 및 형태소 분석 및 생성, 그리고 NLTK에서 구현을 살펴봤다.

2017.02.01

앞의 코드는 동사구의 모든 태그에 관한 정보를 제공한다.
POS 태깅을 통해 얻어지는 단어의 모호성을 보여주는 예제를 살펴보자.
주어진 텍스트에서 튜플 시퀀스가 생성될 수 있는 예제를 살펴보자.
명사 태그 전에 나오는 태그의 수를 계산하는 다음 코드를 살펴보자.
기본 태깅은 모든 토큰에 동일한 품사 태그를 할당하는 일종의 태깅이다.
choose_tag() 메소드는 SequentialBackoffTagger에 의해 구현되야한다.
이제 DefaultTagger의 작업을 설명하는 다음 코드를 살펴보자.
nltk.tag.untag()를 사용하여 태그가 있는 문장을 태그가 없는 문장으로 변환할 수 있다.
NLTK에서 제공하는 POS 태거(tagger)를 사용하여 토큰 목록에 태그를 지정할 수 있다.
NLTK에 의해 제공된 품사 태거(part of speech tagger)를 사용하여 토큰 목록의 태깅을 수행할  수 있다.
